{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa7fb60a",
   "metadata": {},
   "source": [
    "# Assignment 02     \n",
    "## CSCI E-108\n",
    "### Steve Elston\n",
    "\n",
    "\n",
    "In this assignment you will work with some basic streaming analytic algorithms. To avoid the complexities of installing and setting up a real streaming analytics platform, you will work with stream flow data loaded from local files. Specifically in this assignment you will:    \n",
    "1. Create and apply code to perform basic stream queries.    \n",
    "2. Using stream queries and plots, explore the stream data.    \n",
    "3. Use moving windows to compute moving averages and sub-sample a stream.    \n",
    "4. Use exponential decay filters to compute moving averages and sub-sample a stream.    \n",
    "5. Work with a Bloom and quotient filters to filter for customer identifiers on a list.\n",
    "6. Use the count-min-sketch algorithm to find counts of unique event identifiers.\n",
    "7. Use the HyperLogLog algorithm to find the cardinality of events in a stream.  \n",
    "\n",
    "## Overview \n",
    "\n",
    "The United States Geological Survey (USGS) maintains over 13,500 stream flow gauges in the United States. Measurements from most of these gauges are recorded every 15 min and uploaded every 4 hours are [available for download](https://waterdata.usgs.gov/nwis/rt). Stream flow data are used as inputs for complex water management tasks for purposes such as agriculture, residential use and conservation. For this assignment you will work with the time series of measurements for two stream flow gauges on tributaries of the the Columbia River in the US State of Washington.     \n",
    "\n",
    "To get started, execute the code in the cell below to import the packages you will need. \n",
    "\n",
    "> **Note:** You must pip install four packages to run the code in this notebook. You can perform the installation of these packages by uncommenting the shell commands shown in the cell below. Alternatively, you can also Conda install these packages.    \n",
    "> 1. [Mmh3](https://github.com/hajimes/mmh3)      \n",
    "> 2. [Bitarray](https://github.com/ilanschnell/bitarray)\n",
    "> 3. [PyProbables](https://pyprobables.readthedocs.io/en/latest/index.html)\n",
    "> 4. [Datasketch](https://ekzhu.com/datasketch/index.html)\n",
    "> 5. [DataSketches](https://apache.github.io/datasketches-python/5.2.0/index.html)\n",
    ">\n",
    "> The three packages, PyProbables, Datasketch and Apache DataSketches have considerable overlap in the algorithms available. Of the three, Apache DataSketches is considered most suitable for enterprise scale deployments. For example, [DataSketches is integrated with BigQuery](https://github.com/apache/datasketches-bigquery) and [Apache SPARK](https://github.com/apache/datasketches-spark). Unfortunately, the DataSketches' Python API is incomplete and poorly documented. Therefore, we will use several sketch packages in these exercises.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mmh3\n",
    "#!pip install bitarray \n",
    "#!pip install pyprobables\n",
    "#!pip install datasketch\n",
    "#!pip install datasketches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1ad9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import mmh3\n",
    "from bitarray import bitarray\n",
    "from probables import QuotientFilter, HeavyHitters, CountMinSketch\n",
    "from datasketch import HyperLogLog\n",
    "from datasketches import req_floats_sketch, tdigest_float\n",
    "\n",
    "import math\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from decimal import Decimal\n",
    "import sys\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff02ac",
   "metadata": {},
   "source": [
    "## Loading the Dataset  \n",
    "\n",
    "The next step is to load the stream gauge data. The code in the cell below loads the time series data for the first gauge. This gauge is sited on the Okanogan river.  \n",
    "\n",
    "The code in the cell below does the following:  \n",
    "1. Loads the data from a .csv file. \n",
    "2. Converts the time stamp column to an index of the Pandas data frame. \n",
    "3. Assigns human-understandable names to the columns.  \n",
    "4. Returns just the first 4 columns of the data frame. \n",
    "\n",
    "Execute this code and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afbed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_index_series(file_name):  \n",
    "    '''Function to read time series data from a file.\n",
    "    Argument is the path and filename.\n",
    "    Returns a data frame with the file contents'''\n",
    "    df = pd.read_csv(file_name, sep='\\t')\n",
    "    df.index = df.datetime\n",
    "    df.drop('datetime', axis=1, inplace=True)\n",
    "    df = df.iloc[:,:4]\n",
    "    df.columns = ['Agency', 'Site_number', 'Time_zone', 'Stream_flow']\n",
    "    return df.iloc[:,:4]\n",
    "\n",
    "Malott = read_index_series('../data/12447200_Okanogan_at_Malott.txt')\n",
    "Malott"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12aa937",
   "metadata": {},
   "source": [
    "The other time series is for a gauge on the Yakima River. Execute the code in the cell below and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ed6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "CleElm = read_index_series('../data/12479500_Yakima_At_CleElm.txt')\n",
    "CleElm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7b2f4",
   "metadata": {},
   "source": [
    "Since we really only want to work with one data frame. The code in the cell below merges the two time series and sorts them into time index order. Execute this code and examine the result, paying attention to the site number and the datetime index.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ccdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_flow = pd.concat([Malott,CleElm]).sort_index()\n",
    "stream_flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fb8b33",
   "metadata": {},
   "source": [
    "## Querying Stream Data\n",
    "\n",
    "Common stream data operations are often formulated as queries on the stream data. Many streaming data platforms use extensions of SQL for these queries.   \n",
    "\n",
    "To keep things simple in this assignment we use a basic query function. The function shown in the cell below supports queries on the time series. This function calls a function that returns time slices of the stream data frame.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f66133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_dataframe(df, start_time=None, end_time=None):\n",
    "    \"\"\"\n",
    "    Function returns a time slice out of a stream of data\n",
    "\n",
    "    Args:\n",
    "        df: data frame containing the stream data\n",
    "        start_time: the starting time of the slice, can be in datetime form or an integer\n",
    "        end_time: the ending time of the slice, can be in datetime form or an integer\n",
    "    \"\"\"\n",
    "    ## Set the start and end times to the extremes if not specified\n",
    "    if start_time==None: start_time = df.index[0]\n",
    "    if end_time==None: end_time = df.index[df.shape[0]-1]\n",
    "   \n",
    "    ## Test if index is a string datetime or an integer\n",
    "    ## use iloc method if an integer.\n",
    "    ## A slice over the time range is created based on the index type. \n",
    "    if isinstance(df,pd.DataFrame):\n",
    "        if isinstance(start_time, str):\n",
    "            df = df.loc[start_time:end_time,:]\n",
    "        else:     \n",
    "            df = df.iloc[start_time:end_time,:]\n",
    "    else: # Must be a Pandas series \n",
    "        if isinstance(start_time, str):\n",
    "            df = df.loc[start_time:end_time]\n",
    "        else:     \n",
    "            df = df.iloc[start_time:end_time]\n",
    "    return df\n",
    "\n",
    "\n",
    "def query_stream(df, Columns=None, site_numbers=None, start_time=None, end_time=None):    \n",
    "    '''\n",
    "    Function to query the stream gage time series data. \n",
    "    Args:    \n",
    "        df = the data frame containing the data.  \n",
    "        Columns = a list of columns to return.   \n",
    "        site_numbers = a list of gage site numbers to query data. \n",
    "        start_time = the start time of the returned data as datatime string or integer index.   \n",
    "        end_time = the end time of the returned data as datatime string or integer index.\n",
    "    Returns: DataFramew or Pandas series with datatime index and columns sepecified over range of \n",
    "             datetime (index) specified. \n",
    "    '''\n",
    "    ## First set values for arguments set to Null  \n",
    "    if Columns==None: Columns = df.columns\n",
    "    if site_numbers==None: site_numbers = df.Site_number.unique()\n",
    "    \n",
    "    ## Select the sites\n",
    "    df = df.loc[df.Site_number.isin(site_numbers), Columns]\n",
    "\n",
    "    ## A slice over the time range is created based on the arguments. \n",
    "    df = slice_dataframe(df, start_time=start_time, end_time=end_time)\n",
    "    ## Return the results of the query\n",
    "    return df\n",
    "\n",
    "query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc31bf56",
   "metadata": {},
   "source": [
    "You can see the options to run `query_stream` function by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79daad7f",
   "metadata": {},
   "source": [
    "An example of using the query function is shown in the cell below. Execute this code and examine the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af068b6-a869-41b4-a34a-ce7d19dfd547",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0c9f9",
   "metadata": {},
   "source": [
    "> **Exercise 02-01:** Using the `query_stream` function, write and execute the code in the cell below to compute and display the mean `Stream_flow`for the month of April of 2020 of site 12484500. Use the [Pandas.DataFrame.mean](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html) method to compute the mean. Notice that using this approach we can compute most any statistic of interest on the query result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e3173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc98d30",
   "metadata": {},
   "source": [
    "## Plotting Streaming Data\n",
    "\n",
    "Visualization is important tool in data exploration and discovery. Numerical stream data is ideal for visual exploration if it can be subsampled to manageable size.  \n",
    "\n",
    "The function in the cell below creates a time series plot. The time index of the Pandas data frame is used to generate the x-axis values. Execute the code in this cell to load this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(df, ax=None, ylabel='Stream flow', title=''): \n",
    "    if ax==None: fig, ax = plt.subplots(figsize=(20, 6))\n",
    "    df.plot(ax=ax);\n",
    "    ax.set_xlabel('Date');\n",
    "    ax.set_ylabel(ylabel);\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    return ax    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ea571c",
   "metadata": {},
   "source": [
    "The code in the cell below creates time series plots of the stream flow data. The flow time series for two stream gauges queried as arguments to the plot function. Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e82d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12447200]), title='Flow on Okanogan at Malott') \n",
    "_=plot_time_series(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]), title='Flow on Yakima at Cle Elm')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c53ca",
   "metadata": {},
   "source": [
    "The time series of stream flow at both of these gauges is rather complex. Both rivers have several dams used to control the flow. The flow is optimized to conserve fisheries and to supply agriculture in the Columbia River Basin. Water in reservoirs accumulates in the spring as mountain snow melts. The water is then released throughout the spring and summer. \n",
    "\n",
    "But, what can we make of the noticeable spikes in flow, particularly for gauge $12484500$ on the Yakima River. Even with the control provided by dams and reservoirs spring and early summer storm events can cause temporary increases in water flow. These storms bring heavy, and often warm, rain. Flow in the rivers increases not only because of the rainfall, but also since warm rain accelerates snow melt in the higher elevations.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e06c6f",
   "metadata": {},
   "source": [
    "> **Exercise 02-02:** The transitory flow events on the Yakima River warrant some further investigation. You now have the tools to query and plot the stream flow time series. Your goal is to determine if there are common properties (e.g. duration or amplitude) of these events. Plot the results of a query for stream flows on gauge $12484500$, Yakima River, from the 6th day of April to the 20th day of June, 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482f5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b87495",
   "metadata": {},
   "source": [
    "> Discuss any common pattern in terms of approximately common high amplitudes or durations of these flow events you can see. Note, this question is a bit open ended.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ccc59",
   "metadata": {},
   "source": [
    "> **Answer:**               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8589b",
   "metadata": {},
   "source": [
    "## Applying Moving Window Filters\n",
    "\n",
    "Moving window filters are a commonly used method to compute statistical samples from streaming data. \n",
    "\n",
    "Apply a moving window filter. \n",
    "\n",
    "> **Exercise 02-03:** You will complete and test the function in the cell below. The function queries a time series to create overlapping windows of a specified length and stride. For each window the mean of the stream flow is computed. The function returns a [Pandas Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) object. The time index of the Series object is the mid-point index of the window used to compute the statistic. \n",
    "> 1. Define empty lists for the output values and the time index of the output values.\n",
    "> 2. Use the `window_sample` function, including arguments `length` and `stride`, to create a list of the samples over the series of windows. Notice, that for real streaming data, one would work with the continuous stream to create the window samples. Here we use a the Python [yield](https://docs.python.org/3/reference/expressions.html#yield-expressions) as [generator](https://wiki.python.org/moin/Generators) to simulate the stream of (overlapping) window values. Use the default values for `length` and `stride`. The yield declaration allows the function to emit one output at a time.     \n",
    "> 3. Use a `for` loop to create the overlapping moving window samples of the input. The `window_sample` function provided to create a list of window samples using the `length` and `stride` arguments. At each iteration, the window will advance by `stride` time steps.\n",
    "> 4. In a loop over the window samples do the following:\n",
    ">    - Compute the mean of the window sample and append it to the appropriate list.\n",
    ">    - Find the midpoint time index of the window and append it to the appropriate list.   \n",
    "> 6. Once the loop has terminated, use the [Pandas.Series](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html) constructor to instantiate the return series of the mean of the window samples and return this series.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6639360a-d852-4a9c-b344-bd5e50d52275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_sample(ts, \n",
    "                  length=16, \n",
    "                  stride=8): \n",
    "    for ix_end in range(length, len(ts), stride):\n",
    "        ix_start = ix_end - length \n",
    "        yield slice_dataframe(ts, start_time=ix_start, end_time=ix_end)\n",
    "\n",
    "\n",
    "def window_average(ts, length=16, stride=8, Columns='Stream_flow', site_numbers=[12484500]):\n",
    "    half_length = int(length/2)-1\n",
    "\n",
    "    ## Put your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be829a8",
   "metadata": {},
   "source": [
    "> 3. Next you will test your function by completing and executing the code in the cell below. Use your `window_average` function to create a Pandas Series with 4-hour stream flow averages (length of 16 time steps), taken every 2 hours (stride of 8 time steps). Name your return Series `filtered_16. Compute and print the length of the mean filtered series. The code provided queries the data so that you are working with only values from the Yakima River gauge. Using flow rate values from only one gauge simplifies the bookkeeping for window sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Query to create a series with only the Yakima River stream gauge data. \n",
    "Yakima = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])\n",
    "\n",
    "filtered_16 = window_average(Yakima)\n",
    "print(filtered_16.head())\n",
    "len(filtered_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca04cf",
   "metadata": {},
   "source": [
    "> Notice how the length of the time series has been significantly reduced. Is the reduction in length of the time series consistent with a stride of 8 time steps?   \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3992f0",
   "metadata": {},
   "source": [
    "> **Answer:**               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9945f177",
   "metadata": {},
   "source": [
    "To examine the 4-hour moving average time series, the difference series, and the root mean squared error or difference between the two, execute the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plot_time_series(filtered_16, title='Flow on Yakima at Cle Elm. 4-hour average')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d753b0ea-894f-4cb0-9d6a-3688c24cbf67",
   "metadata": {},
   "source": [
    "Notice the following about the results above. The moving average smoother has had only minimal effect on the stream flow time series. The difference series shows only small values with respect to the range of values in the filtered series. Further, the root mean square error (RMSE) is also small compared to the range of values of the filtered series.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd0b31d",
   "metadata": {},
   "source": [
    "> **Exercise 02-4:** You will compute and display a time series using a longer, 1-day (96 time steps) moving window with a stride of 1/2 of a day (48 time steps). You will also perform evaluation of the resulting series. For this exercise, do the following:   \n",
    "> 1. Use the `window_average` function you completed in the previous exercise to compute the smoothed series. \n",
    "> 4. Print the length of the resulting Pandas Series.\n",
    "> 5. Plot the filtered (moving average smoothed) time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ebcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d9f87",
   "metadata": {},
   "source": [
    "> Answer the following questions:   \n",
    "> 1. What is the data compression ratio with respect to the original stream flow series? Is this consistent with the stride of the window?   \n",
    "> 2. Compare the plots of the results of the two moving window summaries. What are the obvious differences?\n",
    "> 3. If the goal is to measure total volume of water passing the gauge on a daily and weekly basis, does the series from the longer filter contain sufficient detail? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f22026",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 1.             \n",
    "> 2.          \n",
    "> 3.                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c4fc4-c96a-4af3-a43d-2df08efe35fe",
   "metadata": {},
   "source": [
    "## Reservoir Sampling   \n",
    "\n",
    "At large scale, one needs to randomly sample from streams. Some of the many examples where working with a random sample of the streams is required include the following.     \n",
    "1. An Internet security system analyzes the IP address of of Internet traffic to detect denial of service attacks. The system needs to compute various statistics on the IP address pair (origin and destination) streams. But, given the massive volume of Internet traffic, it is impossible to store much history of these streams.    \n",
    "2. A large e-commerce company must analyze the click streams on it many web sites. These analytics must consider every click on the sites, not just clicks leading to purchases. It is impractical economically to store long histories of these click streams.\n",
    "\n",
    "Could one use Poisson sampling to reduce the volume of data used for analysis? Let's do some analysis. For basic Poisson sampling an observation is included in the sample with probability $p$. Once we have seen $i$ samples in a stream our sample will be of size $p\\ i$. As time goes on the memory requirement, $m(t)$, continues to increase.   \n",
    "$$m(t) = p\\ i(t) \\rightarrow \\infty\\ as\\ t \\rightarrow \\infty$$\n",
    "It is clear that Poisson sampling is not an option for infinite streams of data! We need another approach.   \n",
    "\n",
    "Reservoir sampling maintains a buffer of constant size $k$. The first $k$ sample to arrived are placed in the buffer. When the $i$th sample arrives a random integer $r(i) \\sim Unif(0,i)$ is generated. If $r(i) \\le k$, the $r$th sample of the buffer is replaced by the new sample. The probability of the $i$th sample being included in the buffer is:   \n",
    "$$p(s(i)) = \\frac{k}{i+1}$$     \n",
    "\n",
    "The code in the cell below computes and displays the size of the required memory and the probability of a sample being included are computed and displayed as a function of $i$. Execute this code and examine the result.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c045253b-06ce-43e0-8ef9-544334e1ee4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_samples = 1000\n",
    "weight = 1.0\n",
    "k=100\n",
    "p=0.01\n",
    "\n",
    "porbability =  lambda i: k * weight / (i + 1) if i > k  else 1\n",
    "length = lambda i: k if i > k else i\n",
    "\n",
    "i_vector = range(N_samples)\n",
    "probabilities = [porbability(i) for i in i_vector]\n",
    "lengths_resivoir = [length(i) for i in i_vector]\n",
    "lengths_poisson = [i for i in i_vector]\n",
    "\n",
    "_, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].plot(i_vector, lengths_resivoir, label='Reservoir Sampling');\n",
    "ax[0].plot(i_vector, lengths_poisson, label='Poisson Sampling');\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel('Number of observations seen');\n",
    "ax[0].set_ylabel('Length of sample vector');\n",
    "ax[0].set_title('Length of sample vs. samples observed');\n",
    "\n",
    "\n",
    "ax[1].plot(i_vector, probabilities, label='Reservoir Sampling');\n",
    "ax[1].plot(i_vector, [p]*len(i_vector), label='Poission Sampling');\n",
    "ax[1].legend()\n",
    "ax[1].set_yscale('log');\n",
    "ax[1].set_xlabel('Number of observations seen');\n",
    "ax[1].set_ylabel('Log probability of inclusion in buffer');\n",
    "ax[1].set_title('Log probabiliy of inclusion vs. samples observed');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eebd4e-02b5-4573-8dad-94cc67f39cd2",
   "metadata": {},
   "source": [
    "Examine these graphs and notice the following, noticing that the probability graph uses a log vertical scale:     \n",
    "- The memory required for Poisson sampling grows linearly with the number of arriving samples.\n",
    "- The memory required for reservoir sampling increases linearly until the buffer is filled and then is constant.\n",
    "- The probability of a sample being included is constant for Poisson sampling.\n",
    "- The probability of a sample being included is constant until the buffer fills and then decreases linearly (exponential on log scale).\n",
    "\n",
    "The foregoing observations show that both sampling methods result in random samples. Reservoir sampling maintains a random sample with fixed memory per stream.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4f2fc-b3d1-4500-afc9-1cf94e1f31fd",
   "metadata": {},
   "source": [
    "### An Efficient Algorithm\n",
    "\n",
    "The basic reservoir sampling algorithm is simple, but not very efficient. A random number needs to be generated for each arriving sample. Random number generation is an expensive process. This simple algorithm scales poorly large numbers of streams and high arrival rates.    \n",
    "\n",
    "A better approach can be obtained by considering the [**geometric probability distribution**](https://en.wikipedia.org/wiki/Geometric_distribution) of the interval between samples. First we generate a random number with the required distribution: \n",
    "\n",
    "$$W = exp \\Bigg( \\frac{log \\big(Unif(0,1) \\big)}{k}   \\Bigg)$$\n",
    "Now the index of the next sample to be placed in the reservoir is sampled:   \n",
    "$$i = i + floor \\Bigg( log \\Big( \\frac{Unif(0,1) }{1-W)} \\Big) \\Bigg) + 1$$\n",
    "\n",
    "Note that when using Python with zero-based indexing, the $+1$ term should be dropped.  \n",
    "\n",
    "The last step is to find the index of the sample replaced in the reservoir from a uniform distribution, $r = Unif(1,k)$. Here $Unif(x,y)$ is the uniform distribution on the interval $[x,y]$ and $floor$ is a function that rounds down to the next lowest integer.       \n",
    "\n",
    "Comparing the foregoing algorithm to the initial simple approach we see that the efficient algorithm needs to compute three random numbers each time a sample is added to the reservoir. However, the three random number need only be computed for samples that are actually placed in the reservoir. Given that the probability of a sample being placed in the reservoir decreases linearly with the number of samples seen in the stream, the number of random numbers generated continues to decrease as the sampling progresses.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafac2ef-af6b-4eab-8dab-eba96cf6d4b4",
   "metadata": {},
   "source": [
    "> **Exercise 2-5:** The code in the cell below implements an efficient version of the reservoir sampling algorithm by the following steps.\n",
    "> 1. The reservoir is filled with the first $k$ samples from the stream.\n",
    "> 2. For the rest of the stream an index for the next sample is computed by a random draw from a Geometric distribution.\n",
    "> 3. Each time a sample is to be added to the reservoir a random draw of an index which determines which sample in the reservoir is replaced.\n",
    ">\n",
    "> Execute this function with the `print_test` and `print_updates` arguments set to `True`.          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac908b9-8072-4aa5-ba0b-81b01e85c3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(stream, k, analytic_function=None, print_test=False, print_updates=False): \n",
    "    \"\"\"\n",
    "    Performs reservoir sampling on a stream of data.\n",
    "\n",
    "    Args:\n",
    "        stream: iterable data stream\n",
    "        k: size of the reservoir\n",
    "        analytic_function: A function that operates on the samples in the reservoir each time a sample is added\n",
    "        print_test: produces voluminous test output when set to True. No analytic results are returned    \n",
    "        print_updates: print number of buffer updates performed on a stream if set to True\n",
    "    \"\"\"\n",
    "    reservoir_buffer = []\n",
    "    out_list = []\n",
    "    out_index = []\n",
    "    n = len(stream)\n",
    "    n_updates = 0\n",
    "\n",
    "    ## First, fill the buffer with the first k observations \n",
    "    for i in range(k):\n",
    "        reservoir_buffer.append(stream.iloc[i])\n",
    "        if print_test: print(reservoir_buffer)\n",
    "        if analytic_function != None: \n",
    "            n_updates += 1\n",
    "            out_list.append(analytic_function(reservoir_buffer))\n",
    "            out_index.append(i)\n",
    "\n",
    "    ## Draw a random number from the distribution  \n",
    "    W = math.exp(math.log(random.random())/k)    \n",
    "    ## Loop over the remaining samples in the stream\n",
    "    while i < n:\n",
    "        i = i + math.floor(math.log(random.random())/math.log(1 - W)) # + 1\n",
    "        if i < n:\n",
    "            indx = random.randint(1, k) -1 \n",
    "            reservoir_buffer[indx] = stream.iloc[i]\n",
    "            W = W * math.exp(math.log(random.random())/k)  \n",
    "            n_updates += 1\n",
    "            if print_test: \n",
    "                print('For sample ' + str(i) + '  with buffer element replaced = ' + str(indx + 1))\n",
    "                print(reservoir_buffer)\n",
    "            if analytic_function != None: \n",
    "                out_list.append(analytic_function(reservoir_buffer))\n",
    "                out_index.append(i)\n",
    "    if print_updates: \n",
    "        print('Number of samples = '+ str(len(stream)))\n",
    "        print('Number of buffer updates = ' + str(n_updates))\n",
    "    if analytic_function != None: return pd.Series(out_list, index=out_index)\n",
    "\n",
    "k = 8\n",
    "Yakima = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500], start_time='2020-04-06 00:00', end_time='2020-04-07 11:59')\n",
    "random.seed(345)\n",
    "reservoir_sampling(Yakima, k, print_test=True, print_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0115e0-479e-4028-8930-1fce8cb7155b",
   "metadata": {},
   "source": [
    "> Examine these results and answer the following questions:      \n",
    "> 1. Do the indices of the samples selected appear to be a sequence with an increasing interval? Keep in mind that for small $n$ (number of samples seen) the Geometric distribution has a high probability of sampling a 0 interval.    \n",
    "> 2. Do the samples replaced in the reservoir buffer appeared to have uniformly random indices?    \n",
    "> 3. Compare the number of samples added to the reservoir buffer to the number of samples in the stream. What does this comparison tell you about the efficiency of this algorithm, which requires three random number draws, to the simple algorithm that requires a random number draw for each arriving sample?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f7994c-e8e3-45f9-857b-430e29d03942",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.        \n",
    "> 2.            \n",
    "> 3.               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb99461-a961-478b-b06f-aab5dd06b896",
   "metadata": {},
   "source": [
    "> The code in the cell below does the following:\n",
    "> 1. Performs a query for the entire Yakima river flow time series.\n",
    "> 2. Performs reservoir sampling on the stream and computes and returns the mean of the sample each time a sample is added to the reservoir buffer.\n",
    ">\n",
    "> Execute this code, noting the number of samples taken and the execution time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bc017-d400-409f-95c9-1867e5f44580",
   "metadata": {},
   "outputs": [],
   "source": [
    "k=64\n",
    "Yakima = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])\n",
    "random.seed(121)\n",
    "%time analytic_series = reservoir_sampling(Yakima, k, lambda x: np.mean(x), print_updates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f13bd8-1936-4594-97e8-05405f17916e",
   "metadata": {},
   "source": [
    "> The code in the cell below computes the mean statistic on the entire expanding window of the stream samples. Execute this code noting the execution time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f813f-d80c-4c7b-a5f1-8cb0869d4ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_full_analytic(stream, index, analytic_function):\n",
    "    out_list = []\n",
    "    for i in index:\n",
    "        out_list.append(analytic_function(stream[:i]))\n",
    "    return pd.Series(out_list, index = index)\n",
    "\n",
    "%time full_analytic = compute_full_analytic(Yakima, analytic_series.index, lambda x: np.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881bac6-38f4-46b1-b247-0cdfc7b665b2",
   "metadata": {},
   "source": [
    "> Finally, to compare the mean statistic computed from the reservoir sample to the statistic computed with the cumulative sample, execute the code in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8ecb1c-4555-419c-87be-3ee4f614a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax=plt.subplots(figsize=(8,6))\n",
    "ax.scatter(analytic_series.index, analytic_series, label='Resivoir sample');\n",
    "ax.plot(analytic_series.index, full_analytic, lw=2, c='orange', label='Full sample');\n",
    "ax.set_title('Mean estimate vs. stream sample index');\n",
    "ax.set_xlabel('Stream sample index');\n",
    "ax.set_ylabel('Mean estimate from reservoir buffer');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860ac38b-cf45-41aa-acc9-edc2251d3618",
   "metadata": {},
   "source": [
    "> Examine the foregoing results and answer these questions:       \n",
    "> 4. How can you account for the for the difference in time required to compute the mean using reservoir sampling versus using the full samples in the expanding window?      \n",
    "> 5. Compare the full sample mean estimates and the mean estimates from the reservoir samples shown in the plots. How would you characterize the reservoir sampling approximation, keeping in mind that we are using a very small buffer with $k=64$ and the values in the stream are not stationary (have a trend)?     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79843e28-beae-42d8-9f37-b020c18be647",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 4.             \n",
    "> 5.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438bb96b",
   "metadata": {},
   "source": [
    "## Exponential Decay Filters\n",
    "\n",
    "The idea of using exponential smooth for time series analysis is an old one, dating at least to use by Weiner in the 1920s. The related idea of moving average filters was developed by Kolmogorov and Zurbenko in the 1940s. Exponential smoothers were used extensively in signal process in the 1940s. The general idea was expanded by Robert Goodell Brown (1956) and C.E. Holt (1957) and his student P.R. Winters (1960). The higher-order Holt-Winters model accounts for trend and seasonality of time series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c336514",
   "metadata": {},
   "source": [
    "### Basic exponential Smoothing\n",
    "\n",
    "Exponential smoothing uses a weighted sum of the current observation and the past smoothed value to compute a new smoothed value. This basic exponential smoothing relationship is shown below.  \n",
    "\n",
    "$$\n",
    "s_0 = x_0\\\\    \n",
    "s_t = \\alpha x_t + (1-\\alpha) s_{t-1} = s_{t-1} + \\alpha(x_t - s_{t-1}),\\ t \\gt 0\n",
    "$$\n",
    "\n",
    "The smoothing hyperparameter, $\\alpha$, controls the trade-off between the last observation and the previous smoothed values. The possible values are in the range, $0 \\le \\alpha \\le 1$. A large value of $\\alpha$ puts more weight on the current observation. Whereas, a small value of $\\alpha$ puts more weight on the smoothed history.      \n",
    "\n",
    "How can we understand the exponential decay of the smoothed history of a time series? The smoothing hyperparameter, $\\alpha$, an be expressed in terms of the decay constant, $\\tau$ and time interval $\\Delta T$ as shown below.  \n",
    "\n",
    "$$\n",
    "\\alpha = 1 - e^{\\big( \\frac{- \\Delta T}{\\tau} \\big)}\n",
    "$$\n",
    "\n",
    "From this relationship you can see that the influence of the smoothed history decays exponentially as $\\delta T$ increases. The decay time increases as $\\tau$ decreases.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e56565",
   "metadata": {},
   "source": [
    "### Smoothing with higher-order terms   \n",
    "\n",
    "The basic exponential smoothing algorithm is effective in many cases. However, the simple first order exponential smoothing method cannot accommodate time series with trend or seasonality. Higher order smoothing models are required.   \n",
    "\n",
    "The **double exponential smoothing** or **Holt-Winters double exponential smoothing** algorithm is a second order smoothing method. Using two coupled difference equations a trend and non-seasonal component of the time series can be modeled. The model updates a smoothed measure of the non-seasonal component and the trend.   \n",
    "\n",
    "The model is initialized with the values:   \n",
    "$$\n",
    "s_1 = x_1 \\\\\n",
    "b_1 = x_2 - x_1\n",
    "$$\n",
    "\n",
    "At each time step the a pair of time difference equations are updated. The following relationships update the smoothed non-seasonal component, $s_t$, and the slope, $b_t$:   \n",
    "\n",
    "$$\n",
    "s_t = \\alpha x_t + (1-\\alpha) (s_{t-1} + b_{t-1}) \\\\\n",
    "b_t = \\beta(s_t - s_{t-1}) + (1 - \\beta)b_{t-1}\n",
    "$$\n",
    "\n",
    "The smoothed non-seasonal component and smoothed slope can be used to compute a forecast. The relationship below computes the forecast $m$ time steps ahead.      \n",
    "\n",
    "$$ F_{t+m} = s_t + m b_t $$   \n",
    "\n",
    "What about seasonal components? A third-order difference relationship can updated a smoothed seasonal component, along with the smoothed non-seasonal and slope components. The details of this process are not discussed further here. The details are available elsewhere, including the [exponential smoothing Wikipedia page](https://en.wikipedia.org/wiki/Exponential_smoothing#:~:text=Exponential%20smoothing%20is%20a%20rule,exponentially%20decreasing%20weights%20over%20time.).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4642b7",
   "metadata": {},
   "source": [
    "### Example of Exponential Decay Filtering     \n",
    "\n",
    "> **Exercise 02-6:** You will now create and test a single or simple exponentially weighted decay filter. This function will have a stride argument just as the window filter function. Your function, `exponential_smooth`, will have arguments of the time series, the exponential smoothing parameter and a stride. In this case the stride $=48$ and $\\tau=24.0$. Your function will do the following:    \n",
    "> 1. Initialize a list for the filtered samples with the first value of the input series. The samples list will contain the exponentially weighted smoothed samples. Make sure you save the first value in the list.   \n",
    ">    a. Initialize empty lists for the output values and time index of the output values.   \n",
    ">    b, Iterates over all the values of the time series starting with the second one. The first observation is the intial value. In the loop update the exponentially weighted smoothed values using the first order smoothing algorithm.\n",
    ">    c. If the loop index modulo the stride $= 0$ then, do the following:     \n",
    ">           - Append the smoothed sample value to the output list.       \n",
    ">           - Append the time index of that sample to the index list.    \n",
    ">    d. Create an output Pandas Series from the output list using the output index list as the index of the series.   \n",
    ">    e. Return the Pandas Series.   \n",
    "> 8. Compute and print the value of $\\alpha$ given $\\tau = 24.0$.   \n",
    "> 9. Execute your function for site number $12484500$ and arguments of the computed $\\alpha$ and stride $=48$, or 12 hours.  \n",
    "> 10. Print the length of the resulting series.     \n",
    "> 11. Print the value of $\\alpha$, given the time step and value of $\\tau$.    \n",
    "> 13. Plot the smoothed series using the `plot_time_series` function.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826c36a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 15/60  # Step time in hours \n",
    "stride = 48 # Stride in samples\n",
    "tau = 24.0 # Decay time in hours \n",
    "def compute_tau(alpha, time_step): \n",
    "    '''Compute the value of tau given the time step and alpha'''\n",
    "    return -time_step / math.log(1 - alpha)\n",
    "def compute_alpha(tau, time_step):  \n",
    "    '''Compute the value of alpha given the time step and tau'''\n",
    "    return 1 - math.exp(-time_step/tau)\n",
    "\n",
    "def exponential_smooth(ts, alpha=0.01, stride=8):\n",
    "    ## Put your code below  \n",
    "    st = ts.iloc[0]\n",
    "    idx = []\n",
    "    out = []\n",
    "    difference = []\n",
    "    ## Put your code below\n",
    "    \n",
    "\n",
    "    \n",
    "    out = pd.Series(out, index=idx)     \n",
    "    return out      \n",
    "\n",
    "alpha = compute_alpha(tau, time_step)\n",
    "smoothed_24 = exponential_smooth(query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500]), alpha=alpha, stride=stride)\n",
    "\n",
    "print('The resulting series length = ' + str(len(smoothed_24)))\n",
    "print('alpha = ' + str(round(alpha, 4)))\n",
    "_=plot_time_series(smoothed_24, title='Flow on Yakima at Cle Elm, EWMA filter, alpha=' + str(round(alpha, 3)) + ', tau=' + str(tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10f002f",
   "metadata": {},
   "source": [
    "> Provide short answers to the following questions:   \n",
    "> 1. Are the number of smoothed samples correct for the stride of the exponential decay filter selected?     \n",
    "> 2. How does the exponentially smoothed series compare to the original series in terms of frequency components?      \n",
    "> 3. Given the value of $\\tau$ (in hours) what does this tell you about the decay length and smoothing of the filter?     \n",
    "> **End of exercise.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf13035",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.           \n",
    "> 2.      \n",
    "> 3.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac1a1de",
   "metadata": {},
   "source": [
    "> **Exercise 02-7:** A question we should ask is what happens if you increase the smoothing constant of the exponential decay filter? In other words, what is the effect of giving greater weight to the most recent values? To find out do the following:  \n",
    "> 1. Compute a value of alpha given $\\tau = 1.0$, one hour.     \n",
    "> 2. Execute the `exponential_smooth` function with arguments of the computed value of $\\alpha$ and `stride=4`.\n",
    "> 3. Print the length of the resulting Pandas Series.\n",
    "> 4. Print the value of $\\alpha$ computed.\n",
    "> 5. Plot the smoothed time series using the `plot_time_series` function.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfc23f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c212c4ba",
   "metadata": {},
   "source": [
    "> Compare the plot of this time series to the previous series less smoothing and provide short answers to the following questions.   \n",
    "> 1. How do the lengths of the smoothed series compare and is this difference consistent with the stride lengths?    \n",
    "> 2. What is the main difference you can see between these smoothed series in terms of frequency components and why is this expected given the decay times?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb66cd2",
   "metadata": {},
   "source": [
    "> **Answers:**   \n",
    "> 1.        \n",
    "> 2.             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1cda9",
   "metadata": {},
   "source": [
    "> **Exercise 02-8:** So far, we have picked a stride that is about 1/2 the decay time, which is a typical choice. The next question to ask is what is the effect of using a longer stride? A longer stride reduces the number of smoothed samples used for further processing. To find out, repeat the calculations and plotting of exercise 2-07, but with stride $=96$ and $\\tau = 24.0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5b74bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 24.0   \n",
    "stride = 96\n",
    "\n",
    "## Put your code below\n",
    "\n",
    "\n",
    "\n",
    "print('Thw length of the resulting series = '  + str(len(smoothed_24_96)))\n",
    "print('alpha = ' + str(round(alpha, 4)))\n",
    "_=plot_time_series(smoothed_24_96, title='Flow on Yakima at Cle Elm, EWMA filter, alpha=0.01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb7ad21",
   "metadata": {},
   "source": [
    "> Compare the result you just created with that from Exercise 2-7 and answer the following questions:\n",
    "> 1. Is the filter series significantly different, or nearly the same as the series with shorter stride?\n",
    "> 2. Why do you think foregoing outcome is expected?        \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4bc6b6",
   "metadata": {},
   "source": [
    "> **Answers:**           \n",
    "> 1.         \n",
    "> 2.              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035d4e8e",
   "metadata": {},
   "source": [
    "## Filtering Discrete Events  \n",
    "\n",
    "In many cases our goal is to filter discrete events based on some type of identifier. The identifier is a hash of nearly any hashable data type. Many examples of these data types include, string customers identifiers, numeric event identifiers, IP addresses, email addresses.  \n",
    "\n",
    "One example of such a method is the [**Bloom filter**](https://en.wikipedia.org/wiki/Bloom_filter). A Bloom filter is quite efficient in terms of computing and memory.  However, the Bloom filter does not allow for deleting matches once they are added to the hash table. An alternative is a **[quotient filter](https://en.wikipedia.org/wiki/Quotient_filter)**. The quotient filter keeps a count of events for each hash. As a result, an event identifier can be removed from the table by decrementing the counts for the hashes. To perform this extra operation the quotient filter uses more memory and is a bit less computationally efficient.   \n",
    "\n",
    "Both Bloom filters and quotient filters operate by the same principle. A hash table of key-values pairs is created. The keys are the hashes of the event identifiers. For a Bloom filter, the value is binary, the event is in the table or it is not. The quotient filter on the other hand, maintains a count of events in each bucket. The count can be incremented as new instances are encountered, or decremented when deleting events.     \n",
    "\n",
    "In the following exercises, you will construct a Bloom filter using a bit array and the Python [mmh3 package](https://github.com/hajimes/mmh3?tab=readme-ov-file). The mmh3 package implements the widely used [MurmurHash (MurmurHash3)](https://en.wikipedia.org/wiki/MurmurHash) non-cryptographic hash algorithm.                      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d85b02",
   "metadata": {},
   "source": [
    "### Instantiate the identifier lists      \n",
    "\n",
    "To start the example, we generate lists of random customer identifiers (IDs) for customers and non-customers by the following steps.     \n",
    "1. Initialize the arguments for the generate of the customer ID lists.  \n",
    "2. Randomly generated lists of customer and non-customer ids are created. A multiplier is used to ensure that some IDs values are outside the range of the length of the hash table. The customer and non-customer identifiers are all integers.   \n",
    "3. The non-customer list is filtered to ensure there are no identifiers common with the customer list.     \n",
    "4. The lengths of the lists and a sample of the customer key values are printed.  \n",
    "\n",
    "Execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_hashes = 1024\n",
    "ID_multiplier = 39\n",
    "number_customers = 100 \n",
    "number_not_customers = 200\n",
    "\n",
    "nr.seed(4565)\n",
    "customers = [int(ID_multiplier * number_of_hashes * i) for i in nr.uniform(size=number_customers)]\n",
    "not_customers = [int(ID_multiplier * number_of_hashes * i) for i in nr.uniform(size=number_not_customers)]\n",
    "\n",
    "## Ensure there are no common ids between customers and non-customers\n",
    "del_list = []\n",
    "for i in range(len(not_customers)): \n",
    "    if not_customers[i] in customers: del_list.append(i)\n",
    "for i in sorted(del_list, reverse=True):\n",
    "    del not_customers[i]\n",
    "\n",
    "print('Number of non-customers = ' + str(len(np.unique(not_customers))))\n",
    "print('Number of customers = ' + str(len(np.unique(customers))))\n",
    "print(customers[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5404b8d",
   "metadata": {},
   "source": [
    "### Using the Murmur Hash\n",
    "\n",
    "You will use the 32 bit Murmur hash function implemented in the Python mmh3 package to create a Bloom filter. First we will investigate some basic properties of the Murmur hash. Specifically, we will investigate how uniform the hash values computed are by the following steps:   \n",
    "1. Created a hash table of length 1024. The table is filled with hashes computed from random integers. Notice that the `mmh3.hash` function only accepts a string argument, requiring the integer to be wrapped in the `str()` function.\n",
    "2. A histogram and cumulative density plot of the hash values are displayed.\n",
    "3. The number of hash collisions is computed and displayed.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191e80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_length = 1024\n",
    "number_samples = 128 \n",
    "multiplier = 1019\n",
    "\n",
    "## Compute random numbers and find the corresponding hash values   \n",
    "samples = [int(multiplier * filter_length * i) for i in nr.uniform(size=number_samples)]\n",
    "hash_list = [mmh3.hash(str(i)) % filter_length for i in samples]\n",
    "\n",
    "## Histogram of the hash values\n",
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "counts, bins, _ = ax.hist(hash_list, bins=filter_length);\n",
    "ax.set_ylabel('Count of keys');\n",
    "ax.set_xlabel('Hash value');\n",
    "ax.set_title('Count of keys vs. hash values');\n",
    "ax.set_xlim(0.0, filter_length)\n",
    "plt.show();\n",
    "\n",
    "## Cumulative density plot of the hash values\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.plot(range(len(counts)), np.cumsum(counts))\n",
    "ax.plot([0.0,len(counts)], [0.0,len(hash_list)], color='red');\n",
    "ax.set_ylabel('Cumulative Sum');\n",
    "ax.set_xlabel('Hash index');\n",
    "ax.set_title('Cumulative sum of hash values');\n",
    "ax.set_xlim(0.0, filter_length)\n",
    "plt.show();  \n",
    "\n",
    "## Compute and display the number of hash collisions\n",
    "print('Number of collisions = ' + str(int(sum([count - 1 if count>1 else 0 for count in counts]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c75ba-e2e8-4167-860f-4d3bbf42f6cc",
   "metadata": {},
   "source": [
    "> **Exercise 02-9:** Compare the plots shown above to the plots of the hash function using a prime number key created for Exercise 01-1. Is the Murmur hash function more or less optimal compared to the function tested in Exercise 1-1 and why?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db54561-1e2e-4582-baca-87fc9d05f9e0",
   "metadata": {},
   "source": [
    "> **Answer:**             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8959eb-650e-431d-869c-1664312157ea",
   "metadata": {},
   "source": [
    "### Create Bloom filter and insert function    \n",
    "\n",
    "You will now instatiate a Bloom filter and create an insert function. As a first step, execute the code in the cell below to instantiate the Bloom filter as an empty bit array.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc96d4b-c2ae-40ff-ae93-e349d7a80f46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filter_length = 1024\n",
    "bloom_filter = bitarray([0]*filter_length)\n",
    "bloom_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77282e20-8b79-47c5-b5b9-b3eaf323943e",
   "metadata": {},
   "source": [
    "> **Exercise 2-10:** You will now complete and test a function to insert an event identifier into the Bloom filter array. To complete the code in the cell below do the following: \n",
    "> 1. Iterate over three hash seeds to create three independent hashes for each key, `x`. The seed is the second argument for the [mmh3.hash function](https://mmh3.readthedocs.io/en/latest/api.html) and is just an integer. Any three integers will work for this case.   \n",
    "> 2. For each seed compute the hash value and take the modulo with the length of the Bloom filter.\n",
    "> 3. Set the bit of the Bloom filter bit array indexed by the hash value to 1.\n",
    "> 4. Return the Bloom filter array.        \n",
    "> Execute your code to add the customers to the hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292d5f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hashes = 3\n",
    "def bloom_insert(x, bloom_filter, n_hashes=3):\n",
    "    ## Put your code below\n",
    "    \n",
    "\n",
    "    \n",
    "    return bloom_filter    \n",
    "\n",
    "for customer in customers:  \n",
    "    bloom_filter = bloom_insert(str(customer), bloom_filter, n_hashes=n_hashes)\n",
    "bloom_filter    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1157d87",
   "metadata": {},
   "source": [
    "> Notice that most bits in the Bloom filter array are still 0, and only a small fraction of the bits set to 1.\n",
    ">\n",
    "> Next, you will complete the `bloom_query` function in the cell below. Recall that a query returns True only if all bits indexed by the hash values are 1. To complete the function, do the following:     \n",
    "> 1. Set a counter variable to 0.     \n",
    "> 2. Iterate over the number of hashes.     \n",
    "> 3. Compute the hash for the input value, `x`, and integer hash key.    \n",
    "> 4. If the bit value indexed for by the key equals 1, add 1 to the counter value.      \n",
    "> 5. If the counter value equals the number of hashes, return True, and if not, return False.     \n",
    "> Now, execute the code in the cell below to test your function and find the number of false positives.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloom_query(x, bloom_filter, n_hashes=3):   \n",
    "    ## Put your code below  \n",
    "    \n",
    "    \n",
    "\n",
    "false_positives = 0 \n",
    "for customer in not_customers:  \n",
    "    if bloom_query(str(customer), bloom_filter, n_hashes=n_hashes): false_positives += 1\n",
    "print('Total number of false positives = ' + str(false_positives))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0663164c",
   "metadata": {},
   "source": [
    "> For a perfect (uniformly distributed) hash function the theoretical false positive rate can be computed by the following relationship:  > $$P(false\\ positive) = \\Big[1 - exp \\big(\\frac{- k n}{m} \\big) \\Big]^k$$    \n",
    "> Where, \n",
    "> - $k = $ number of hash functions.    \n",
    "> - $m = $ length of the hash table.   \n",
    "> - $n = $ number of identifiers in the hash table.\n",
    ">  \n",
    "> Now answer the following questions:     \n",
    "> 1. Given this result, what is the empirical false positive rate?     \n",
    "> 2. How does the observed false positive rate compare to the theoretical rate? You should create a function to compute the theoretical false positive rate, which you can use for the next exercise. Name your function `false_positive_rate` with arguments, number of events, length of the bit array, and number of hashes.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d14642",
   "metadata": {},
   "source": [
    "> **Answers:** \n",
    "> 1.        \n",
    "> 2.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a417e7-2a5b-4900-964e-15043e972860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_rate(n, m, k):\n",
    "    return (1 - math.exp(-k * n / m))**k \n",
    "false_positive_rate(number_customers, filter_length, n_hashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a136d0-db91-4a0f-811c-5fb464f2c747",
   "metadata": {},
   "source": [
    "> Answer 2 Continued; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426142f8-dc7e-42a0-8430-f4d0be74ed78",
   "metadata": {},
   "source": [
    "> Next, execute the code in the cell below that uses your `false_positive_rate` function to plot log false positive rates for a number of filter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6aa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_list = [2**12, 2**16, 2**24, 2**32]\n",
    "k_list = [2**2, 2**4, 2**6, 2**8, 2**10]\n",
    "n = 10e+7\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "for k in k_list:    \n",
    "    fp_list = [false_positive_rate(n, m, k) for m in m_list]\n",
    "    ax.plot(m_list, fp_list, label=str(k) + ' hashes');\n",
    "#    ax.plot(k_list, fp_list, label=\"{:.1E}\".format(Decimal(m)) + ' bit array');\n",
    "ax.set_yscale('log');\n",
    "ax.set_ylabel('Log false positive rate');\n",
    "ax.set_xlabel('Length of bit array');\n",
    "ax.set_title('False postive rate vs. length of bit array for 10 million events');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0343ce1d",
   "metadata": {},
   "source": [
    "> Finally, answer these questions:    \n",
    "> 3. What is the relationship between length of the bit array and false error rate, and why do you think this makes sense?     \n",
    "> 4. How does increasing the number of hashes affect the false positive rates, and why do you think this makes sense?       \n",
    "> 5. Describe how you can merge two Bloom filters.       \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c4116a",
   "metadata": {},
   "source": [
    "> **Answers:**     \n",
    "> 3.          \n",
    "> 4.        \n",
    "> 5.             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa95248-baba-4fe9-a748-4cc610238082",
   "metadata": {},
   "source": [
    "## Applying the Quotient Filter\n",
    "\n",
    "The quotient filter is an improvement on a Bloom filter, and uses a more compact hashing method. Like Bloom filters, quotient filters are not particularly difficult to implement. There are also several Python packages that support Bloom and quotient filters. For the following exercise you will use the [PyProbabilies](https://pyprobables.readthedocs.io/en/latest/index.html) package.     \n",
    "\n",
    "> **Exercise 02-11:** In this exercise you will work with a quotient filter. To instantiate and add hashes for customer IDs do the following:   \n",
    "> 1. Instantiate the quotient filter using the [QuotientFilter](https://pyprobables.readthedocs.io/en/latest/code.html#probables.QuotientFilter) function using default argument values. Name your filter object `quotient_filter`.      \n",
    "> 2. To get a feel for the quotient filter created print the following attributes of the filter object, `quotient`, `remainder` and `num_elements`.     \n",
    "> 3. Iterate over the customer list and add the customer identifier to the quotient filter using the `add` method. Note that the key argument must be of a string type.\n",
    "> 4. Print the `elements_added` attribute of the quotient filter.\n",
    "> Execute your code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6d6a6-0e50-45ae-b7c5-9a77be272221",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c1c2f-d596-4f13-aaa9-7f39bfa9c434",
   "metadata": {},
   "source": [
    "> Next, you will query the quotient filter with the identifiers in the not_customer list, by executing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb917807-4331-4811-9eda-232d22321fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "false_positives = 0 \n",
    "for customer in not_customers:  \n",
    "    if quotient_filter.check(str(customer)): false_positives += 1\n",
    "print('Total number of false positives = ' + str(false_positives))        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c04e25-1bd4-4c3a-aaca-7e69ce29ebbe",
   "metadata": {},
   "source": [
    "> Compare the false positive rates of the Bloom filter example of Exercise 2-9 and the quotient filter. Can you see that one filter has an advantage for this case and why?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f83730-d9df-4234-9fe9-2faa887ee80d",
   "metadata": {},
   "source": [
    "> **Answer:**              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491f577-a9b2-4456-a068-90808a66a4a8",
   "metadata": {},
   "source": [
    "## Counting Events with \n",
    "\n",
    "Counting discrete events is a fundamental algorithm for stream processing. Naive algorithms require large amounts of memory to maintain a table of counts for each event identifier. Instead, we use sketch algorithms, in particular count-min-sketch.\n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law) is generally considered a model for activity of online user activity, sport event attendance, and other human activities. It is also thought that the frequency of occurrence of words in strings. One possible model for this behavior is the [Zipf-Mandelbrot distribution](https://en.wikipedia.org/wiki/Zipf%E2%80%93Mandelbrot_law), with the following **probability mass function**:   \n",
    "\n",
    "$$f(k,N,\\alpha,\\beta)=\\frac{1/(k + \\beta)^\\alpha}{H_{N,q,s}}$$\n",
    "where:   \n",
    "$k=$ rank in set of size $N$.       \n",
    "$\\alpha$ abd $\\beta$ are parameters.     \n",
    "$H_{N,q,s}=\\sum^N_{k=1} 1/(k + \\beta)^\\alpha$\n",
    "\n",
    "Execute the code in the cell below to sample from the Zipf-Mandelbrot distribution and display a density plot.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d1956-91e5-4c08-85c0-9595e427e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.0\n",
    "beta = 2.7\n",
    "zipf = [1/(i + beta)**alpha for i in range(len(customers))]\n",
    "zipf = np.divide(zipf, np.sum(zipf))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.plot(range(len(customers)), zipf)\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.set_xlabel('Rank')\n",
    "ax.set_title('Density of Zipf distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23b6f23-f77b-4342-b0b3-723ce8ff0c60",
   "metadata": {},
   "source": [
    "Notice the rapid decay of this distribution with event identifier. This rapid decay of event frequency is often observed in many real-world phenomenon.    \n",
    "\n",
    "Next, execute the code in the cell below to generate a data set of identifiers drawn from the Zipf-Mandelbrot distribution of events just computed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1740e8-e8e4-4e79-8022-a6b2a49d91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = int(10e+5)\n",
    "stream = np.random.choice(customers, size=n_samples, p=zipf)\n",
    "stream[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53539948-0b39-4037-b6a3-430140825498",
   "metadata": {},
   "source": [
    "Notice the random nature of the identifiers in the first 200 events in the stream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6845ae8e-6c9c-4a81-aeeb-7d9588051180",
   "metadata": {},
   "source": [
    "### The Count-Min-Sketch Algorithm \n",
    "\n",
    "The probabilistic count min sketch algorithm uses multiple hash functions in an efficient data structure to estimate \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0b2bf0-de0c-4966-b25e-cebe0ba5fb41",
   "metadata": {},
   "source": [
    "> **Exercise 02-12:** You will now use the [probables.HeavyHitters](https://pyprobables.readthedocs.io/en/latest/code.html) function to find the 20 event identifiers with the highest event count. Do the following:\n",
    "> 1. Compute the required width of the data structure to achieve an approximate 1% error rate using the following relationship:     \n",
    ">    $$width = 2^{log_2(e/\\epsilon)}$$       \n",
    ">    To perform this calculation use the Python `math.log2` function and the `math.ceil` function to round up the next highest integer. Display the computed result\n",
    "> 2. Compute the required depth to ensure a less than 1% probability, $\\delta$, of breaching the error bound using the following relationship:\n",
    ">    $$depth = ln(1/\\delta)$$\n",
    ">    Use the `math.ceil` function to round up to the next highest integer. Display the computed result.    \n",
    ">\n",
    "> Execute the code that creates a list of count-min-sketches using the [probables.CountMinSketch](https://pyprobables.readthedocs.io/en/latest/code.html#countminsketch) by these steps.\n",
    "> - A function is instantiated to create a count-min-sketch from a 'stream' of event identifiers with the specified width and depth. The `add` method is used to add each single instances of each identifier in the stream to the sketch.\n",
    "> - A list of count-min-sketches is created for 5 segments of the overall stream.\n",
    "> - The first 20 identifiers and counts for the first of the sketches in the list is printed.       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708c9405-7f61-42dc-9d33-07aa8a99f2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0.01\n",
    "delta = 0.01\n",
    "## Put your code below  \n",
    "\n",
    "\n",
    "\n",
    "print(f\"Sketch has width: {width}  and depth:{depth}\")\n",
    "\n",
    "def build_CountMinSketch(stream, width, depth): \n",
    "    cms = CountMinSketch(width=width, depth=depth)\n",
    "    for customer in stream: \n",
    "        cms.add(str(customer), 1)\n",
    "    return cms\n",
    "\n",
    "sketch_list = []\n",
    "stream_length = 20000\n",
    "for end in range(20000, 120000, stream_length): \n",
    "    start = end - stream_length\n",
    "    sketch_list.append(build_CountMinSketch(stream[start:end], width, depth))\n",
    "\n",
    "for customer in list(stream)[:20]: \n",
    "    print(f'count for customer {customer}: {sketch_list[0].check(str(customer))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc84ff-0058-4670-966d-a684a6f1a70c",
   "metadata": {},
   "source": [
    "> Notice the variation in the number of events for these identifiers.  \n",
    ">\n",
    "> Merging is an essential operation for sketches. The PyProbables CountMinSketch uses the `join` method on pairs of sketches. In this case, merging the 5 sub-stream sketches creates a sketch for the entire stream length. The results for the first 50 identifiers is then printed.\n",
    ">\n",
    "> Execute this code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3dd78-6a83-4aec-b954-1f195b6e2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sketch = sketch_list[0]\n",
    "for i in range(1,len(sketch_list)):\n",
    "    long_sketch.join(sketch_list[i])\n",
    "\n",
    "for customer in list(stream)[:20]: \n",
    "    print(f'count for customer {customer}: {long_sketch.check(str(customer))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a9a3bb-fd14-450e-8a1d-4e2c8db8cfec",
   "metadata": {},
   "source": [
    "> Examine the foregoing results and answer these questions.\n",
    "> 1. Is the merged sketch the same size as the sub-stream sketches? Explain your reasoning.        \n",
    "> 2. Compare the counts for the identifiers shown for the single sub-stream and the entire stream makes sense and how can you explain these differences?\n",
    "> 3. Compute the required memory for the count-min-hash data structure, assuming 4-byte (32 bit) counters.      \n",
    "> 4. How does the memory required for the count-min-hash data structure compare to using a simple linear (or flat) table for the $10^5$ event identifiers, assuming 4-byte (32 bit) counters? Keeping in mind that $10^5$ event identifiers, what does your comparison mean for the memory efficiency of the count-min-sketch algorithm?\n",
    "> 5. If smaller values for $\\epsilon$ and $\\delta$ are selected, how is the size of the sketch changed and why?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436b053-837f-4e02-8a58-de403fd3c8fe",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.          \n",
    "> 2.           \n",
    "> 3.           \n",
    "> 4.         \n",
    "> 5.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e29e10-b042-4260-9525-10403833634d",
   "metadata": {},
   "source": [
    "### The Heavy Hitters Algorithms\n",
    "\n",
    "A variation on the count-min-sketch algorithm is used to find the **heavy hitters**, or the event ids that occur most frequently. This approach allows one to find the counts of a pre-determined number of common events, using the small memory footprint of the count-min-sketch algorithm, but at a cost of some additional computation for sorting and bookkeeping.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640f6bf3-8ee7-43ca-a760-f0b7706a2f7d",
   "metadata": {},
   "source": [
    "> **Exercise 2-13:** You will now apply the heavy hitters algorithm to the event identifier stream. The code in the cell below does the following.  \n",
    "> 1. Instantiates a HeavyHitters object with the arguments computed above and `num_hitters = 20`.    \n",
    "> 2. Iterates over the events in the stream applying the `add` method for each event, with the num_els argument set to 1.      \n",
    "> 3. Save the `heavy_hitters` attribute to a variable. The data structure for this attribute is a dictionary.\n",
    ">\n",
    "> Execute this code   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa6a57-bb87-40a1-a61c-33b4ec8af8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heavy_hitters = HeavyHitters(num_hitters=20, width=width, depth=depth)\n",
    "for event in stream:   \n",
    "    heavy_hitters.add(str(event), num_els=1)\n",
    "\n",
    "heavy_hitters = heavy_hitters.heavy_hitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf278549-14c5-4d03-bd8e-b39472a07ea4",
   "metadata": {},
   "source": [
    "> Now, execute the provided function, using the `heavy_hitters` sketch created above.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb680e29-0953-4bf9-b444-c2250cf4cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dist(hitters):\n",
    "    heavy_hitters = np.flip(np.sort(list(hitters.values()))) \n",
    "    print('The ordered list of heavy hitters:')\n",
    "    print(heavy_hitters)  \n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    ax.plot([x + 1 for x in range(len(heavy_hitters))], heavy_hitters)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_xlabel('Ordered event')\n",
    "    ax.set_title('Ordered event frequencies')\n",
    "\n",
    "display_dist(heavy_hitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930ed908-56b8-400f-a953-90ffd88bdf3b",
   "metadata": {},
   "source": [
    "> Does the distribution of event counts for the top 20 identifiers appear to follow the expected distribution?      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162309c2-6c3d-48ad-8540-5ff7afdf2cf4",
   "metadata": {},
   "source": [
    "> **Answer:**               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06b9bd6-dbf8-4920-b5bd-8406a6c174ec",
   "metadata": {},
   "source": [
    "## HyperLogLog Algorithm   \n",
    "\n",
    "The HyperLogLog algorithm and its derivatives are fast and efficient methods for determining the cardinality of events in streams. The HyperLogLog uses the harmonic mean of a set of cardinality estimates. Each estimate is based on a hash-based sketch of cardinality.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d602f-70b1-442f-86a5-161dd18a49de",
   "metadata": {},
   "source": [
    "> **Exercise 02-14:** You will now work with the HyperLogLog algorithm using the Python [Datasketch package](https://ekzhu.com/datasketch/index.html). Do the following:      \n",
    "> 1. Start with the typical value of $p=14$ (confusingly precision by the creators of the DataSketch package), resulting in the number of registers, $m = 2^{14}$.     \n",
    "> 2. Compute and display the expected error rate, $\\epsilon$, given the value of $p$.      \n",
    "> 3. Instantiate a HyperLogLog object using the value of $p$ previously specified as the argument.      \n",
    "> 4. Assuming that 1-byte registers are used for accumulating the counts, compute and display the size of the HyperLogLog data structure. You can find the size of this data structure using the Python `len` function.           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e20e9e-50ac-4c5f-8227-b1accd36508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8751db-a106-4c05-be87-da7db61a4c99",
   "metadata": {},
   "source": [
    "> The HyperLogLog sketch will now be applied to 10 segments of the event stream by the following steps:     \n",
    "> 1. A function is instantiated to create a HLL sketch from a stream of event identifiers.\n",
    "> 2. The full stream is iterated over in 10. segments. A HLL sketch is appended to the list for each segment.\n",
    "> 3. The estimated cardinality is printed.\n",
    ">   \n",
    "> Execute this code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfbafc8-1a31-4670-8d5d-9049af083dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HLL_sketch(stream, p): \n",
    "    sketch = HyperLogLog(p) \n",
    "    for event in stream: \n",
    "        sketch.update(event)   \n",
    "    return sketch\n",
    "\n",
    "sketch_list = []\n",
    "stream_length = 10000\n",
    "for end in range(10000, 110000, stream_length): \n",
    "    start = end - stream_length\n",
    "    sketch_list.append(HLL_sketch(stream[start:end], p))\n",
    "\n",
    "print(f\"The cardinality of the first stream segment: {round(sketch_list[0].count(), 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80402a94-3fdf-4e06-b025-c2442914e685",
   "metadata": {},
   "source": [
    "> The ability to merge HypterLogLog sketches is an important attribute for stream processing at a massive scale. HLL sketches of multiple streams can be computed in parallel and then merged. Alternatively, HLL sketches can be computed for short segments of a stream and then merged to form sketches of longer segments.\n",
    ">\n",
    "> The code in the cell below merges the HLL sketches for the 10 segments of the stream and then prints the results. Execute this code.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5164ea-2404-4723-837f-66145498d045",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_HLL_sketch = sketch_list[0]\n",
    "for i in range(1,len(sketch_list)):\n",
    "    long_HLL_sketch = HyperLogLog.union(long_HLL_sketch, sketch_list[i])\n",
    "\n",
    "print(f\"The cardinality of the full stream: {round(long_HLL_sketch.count(), 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3ab4e7-3e4a-4012-b486-afff573bd94b",
   "metadata": {},
   "source": [
    "> Now, answer these questions:     \n",
    "> 1. Based on the estimated error and the estimated cardinality value computed, is the error within the expected range for both the stream segment and the full stream?   \n",
    "> 2. How much memory would be required if you needed to find the cardinality of $2^{14}=16384$ streams in a large scale application?\n",
    "> 3. Explain why the merged sketch must be the same size as the sketches for the segments.      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b564757-3096-487e-8b3c-898a020506a9",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.             \n",
    "> 2.    \n",
    "> 3.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a891b5-7853-4172-8402-d23dccb8923b",
   "metadata": {},
   "source": [
    "## Quantile Estimation \n",
    "\n",
    "The empirical estimation of quantiles of unknown distributions is a fundamental problem in data mining. Quantiles estimates have many uses in data mining. One example is a median estimate for some time period of a stream. As another example, one can use extreme quantiles of time intervals of a stream to monitor for anomalies. There are many other applications. As yet another example, quantile estimation is used to evaluate performance of networks and server clusters as well as for cybersecurity monitoring.        \n",
    "\n",
    "For the examples in this notebook we will apply two of the many possible algorithms quantile estimation sketch algorithms; the widely used [t-digest algorithm of Dunning and Ertl, 2019](https://arxiv.org/abs/1902.04023), and the [Relative Error Streaming Quantile algorithm of Cormode, et. al., 2019](https://arxiv.org/abs/2004.01668).    \n",
    "\n",
    "These quantile estimation sketches are **mergeable**. As a result, one can construct a hierarchy of time intervals from shorter to longer.   \n",
    "\n",
    "For the exercises in this notebook you will apply both of these algorithms to the one of the stream flow time series. You will apply this the quantile estimation sketches to 24 hour periods. These 24 hours sketches will be merged into 20 day intervals.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5e027-d2fc-42dd-af4b-0d413b2a1f0f",
   "metadata": {},
   "source": [
    "### Quantile estimate with t-digest   \n",
    "\n",
    "The [**t-digest sketch algorithm**}(https://arxiv.org/abs/1902.04023) is based on an heuristic model. Consequently, there are no know analytical error bounds on the performance of the t-digest algorithm. Empirical tests show that the t-digest algorithm reliably produces accurate results, particularly at extreme quantiles.          \n",
    "\n",
    "In summary, the algorithm has two major components.   \n",
    "1. A sampling scheme using **scale functions** to construct the sketch. The scale function favor samples at extreme values, thereby reducing the error of estimating extreme quantiles at the expense of accuracy of the mid-range quantiles. The scale function has one parameter, $\\delta$, which controls the number of centroids. Larger $\\delta$ leads to a larger sketch with more accurate quantile estimates.    \n",
    "2. A complex interpolation scheme is used between the centroids to improve accuracy of the quantile estimates.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d13f1e-2d3e-436b-89fd-442e9d3c56ad",
   "metadata": {},
   "source": [
    "> **Exercise 02-15:** You will now use a t-digest to find quantiles of a the stream flow time series using the [Apache DataSketches t-digest sketch](https://apache.github.io/datasketches-python/5.2.0/quantiles/tdigest.html). Now do the following:\n",
    "> 1. Instantiate a `tdigest_float` object with argument `k=100`. Make sure you return an int type from your function. \n",
    "> 2. Iterate over the values in `stream_ts` applying the `update` method for each value\n",
    "> 3. Print the quantile values for $q=0.001$ and $q=0.999$ using the `get_quantile` method.\n",
    ">\n",
    "> Execute your code      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a73af-ca29-4b2b-a021-6491461eb6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_ts = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])\n",
    "\n",
    "## Put your code below \n",
    "## instantiate the t-digest \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec2b8eb-ff60-42c0-8caa-b2ad97c14fed",
   "metadata": {},
   "source": [
    "> Notice the rage of values of these quantiles.\n",
    ">  \n",
    "> The code in the cell below does the following:      \n",
    "> 1. The `t_build_digest` function adds the values to the sketch.     \n",
    "> 2. The `merge_sketches` function iterates over a list of sketches and merges them. The function also returns an empty list to accumulate the next sketch list.     \n",
    "> 3. The main function, `t_digests_example` does the following:       \n",
    ">    a. Instantiates a generator for the data to efficiently iterate through windowed samples of the data.    \n",
    ">    b. Iterate over the time windows and create a digest using the t_digest_build function and quantile estimate for each.     \n",
    ">    c. If the number of windows = n_periods, then merge the digests and compute a quantile estimate for the longer period.     \n",
    "> 4. The results are plotted for the quantiles specified.  \n",
    ">\n",
    "> Execute this code   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f84c03a-ee5d-4f2d-98c4-835eec48d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_quantiles(q_time_series, long_q_time_series, q, period, ax):\n",
    "    ax.plot(range(len(q_time_series)), q_time_series, label = 'weekly q = ' + str(q));\n",
    "    x_vals = [x - period/2 for x in range(0, len(q_time_series), period)[1:]]\n",
    "    ax.scatter(x_vals, long_q_time_series, label = 'monthly q = ' + str(q));\n",
    "\n",
    "def plot_mulitple_quantiles_t(ts, q_list, k, period, num_samples):  \n",
    "    \"\"\"\n",
    "    Function to plot quantiles for daily and 20 day periods for the time series, ts, from a list of quantile values, q_list\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(figsize=(8,4))\n",
    "    for q in q_list:   \n",
    "        q_time_series, long_q_time_series = t_digests_example(stream_ts, q, k, period, num_samples)\n",
    "        plot_quantiles(q_time_series, long_q_time_series, q, period, ax)\n",
    "    plt.legend()\n",
    "    ax.set_xlabel('Days')\n",
    "    ax.set_ylabel('Quantile')\n",
    "    ax.set_title(f'Quatile vs. sample number')\n",
    "\n",
    "\n",
    "def sketch_build(sketch, ts):\n",
    "    for val in ts: sketch.update(val) \n",
    "    return sketch\n",
    "\n",
    "def merge_sketches(sketch_list):\n",
    "    long_sketch = sketch_list[0]\n",
    "    for sketch in sketch_list[1:]:\n",
    "        long_sketch.merge(sketch)\n",
    "    sketch_list = []\n",
    "    return long_sketch, sketch_list\n",
    "\n",
    "def t_digests_example(window_samples, q, k, n_days, num_samples):\n",
    "    # Instantiate the generator \n",
    "    window_samples = window_sample(stream_ts, \n",
    "                  length=num_samples, \n",
    "                  stride=num_samples)\n",
    "\n",
    "    # Initialize the lists  \n",
    "    digests = []\n",
    "    q_time_series = []\n",
    "    long_q_time_series = []\n",
    "    \n",
    "    # Build the daily digests and estimate the quantile\n",
    "    day_count = 1  \n",
    "    for window in window_samples: \n",
    "        t_digest = tdigest_float(k=k)\n",
    "        temp_digest = sketch_build(t_digest, window)\n",
    "        digests.append(temp_digest)\n",
    "        q_time_series.append(temp_digest.get_quantile(q))\n",
    "        # If we have enough time steps merge the digests and clear the digests list\n",
    "        if day_count % n_periods == 0: \n",
    "            long_digest, digests = merge_sketches(digests)\n",
    "            long_q_time_series.append(long_digest.get_quantile(q))\n",
    "        day_count += 1\n",
    "    return q_time_series, long_q_time_series\n",
    "\n",
    "# Plot the results for multiple quantiles \n",
    "num_samples = 7*4*24\n",
    "q=0.001\n",
    "day_count = 1\n",
    "n_periods = 4\n",
    "q_list = [0.001, 0.999]\n",
    "plot_mulitple_quantiles_t(stream_ts, q_list, k, n_periods, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72c057d-3ebe-48b9-a15e-3b5791b39bcb",
   "metadata": {},
   "source": [
    "> Examine the results and answer the following questions.\n",
    "> 1. As a result of the weekly window having only $7 \\times 24 \\times 4= 682$ samples, a small value of the hyperparamter K has been used. In one or two sentences describe the effect of this choice on the accuracy of the quantile estimates.\n",
    "> 2. Careful examination of the difference between $q=0.001$ and $q=0.999$ daily quantiles displayed on the graph shows that the difference between these quantiles change over time. In one or a few sentences describe the relationship between these quantiles and the behavior of the stream flow.\n",
    "> 3. Notice the monthly $q=0.001$ and $q=0.999$ quantiles. In one or a few sentences describe the relationship between the monthly and weekly quantiles and are this relationship expected and why.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b375335-b2a8-4b2d-860c-d4b14136353d",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.  \n",
    "> 2.         \n",
    "> 3.                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbe4843-75e2-4298-b84f-7c3a8b26988d",
   "metadata": {},
   "source": [
    "### Quantile Estimation with REQ Sketch\n",
    "\n",
    "The [Relative Error Streaming Quantiles (REQ)](https://arxiv.org/pdf/2004.01668) sketch provides a highly efficient algorithm with bounded error. The REQ algorithm is constructed from a stack of **compactors** with increasing buffer capacity. The buffers of length $B$ are sampled as follows:   \n",
    "1. The smallest (or largest) $b/2$ values in the buffer are retained. This approach retains the most important samples for determining extreme quantiles are retained.\n",
    "2. The largest (or smallest) $b/2$ values are divided into $k$ buckets of ascending (or descending) order. The buckets are then sampled from a deterministic [**Geometric Distribution**](https://en.wikipedia.org/wiki/Geometric_distribution). This approach again ensures accuracy for extreme quantiles while limiting the size of the sketch.\n",
    "\n",
    "The **relative error**, $\\epsilon$, of the quantile estimate, $\\hat{R}(q)$, is measured with respect to the true quantile value, $R(q)$ is written:   \n",
    "\n",
    "$$|\\hat{R}(q) - R(q)| \\le \\epsilon R(q)$$\n",
    "\n",
    "The parameter $k$ is computed for an given relative error, $\\epsilon$, and a probability of exceeding the error bound, $\\delta$, by the following relationship:   \n",
    "\n",
    "$$k = 2 \\Bigg[\\frac{4}{\\epsilon} \\sqrt{\\frac{log(1/\\delta)}{log_2(\\epsilon n)}} \\Bigg]$$\n",
    "\n",
    "The required buffer size is then:  \n",
    "\n",
    "$$B = 2k\\ log_2(n/k)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4839e3-65e6-4552-b859-13b5d3f25383",
   "metadata": {},
   "source": [
    "> **Exercise 02-16:** You will now use a REQ sketch to find quantiles of a the stream flow time series using the [Apache DataSketches Relative Error Quantile sketch](https://apache.github.io/datasketches-python/5.2.0/quantiles/req.html#). Now do the following:\n",
    "> 1. Complete the code in the function to compute the parameter k. \n",
    "> 2. Instantiate a `req_sketch` object with argument `k=100`. \n",
    "> 3. Iterate over the values in `stream_ts` applying the `update` method for each value\n",
    "> 4. Print the quantile values for $q=0.001$ and $q=0.999$ using the `get_quantile` method.\n",
    ">\n",
    "> Execute your code    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7be6dc3-b08b-4368-adc8-7e0a190b6611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to compute k\n",
    "eps = 0.05\n",
    "delta = 0.01    \n",
    "n = 180*4*24\n",
    " \n",
    "def compute_K(eps, delta, n):\n",
    "    ## Put your code below\n",
    "    \n",
    "    \n",
    "\n",
    "k = compute_K(eps, delta, n)\n",
    "print(f\"Value of k: {k}\")\n",
    "\n",
    "stream_ts = query_stream(stream_flow, Columns='Stream_flow', site_numbers=[12484500])\n",
    "\n",
    "req_sketch = req_floats_sketch(k=k)\n",
    "for val in stream_ts: \n",
    "    req_sketch.update(val)\n",
    "\n",
    "print(f\"Lower quantile: {req_sketch.get_quantile(0.001)}\")\n",
    "print(f\"Upper quantile: {req_sketch.get_quantile(0.999)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e1917-efe7-465e-8b63-437af032a77d",
   "metadata": {},
   "source": [
    "> The code in the cell below executes and displays the results for weekly and monthly quantile estimates using the REQ algorithm. The organization of the code is nearly identical to the code used for the t-digest quantile analysis. Execute this code.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1d601-63a0-436c-b63b-07e49895a200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mulitple_quantiles_req(ts, q_list, k, period, num_samples):  \n",
    "    \"\"\"\n",
    "    Function to plot quantiles for daily and 20 day periods for the time series, ts, from a list of quantile values, q_list\n",
    "   \"\"\"\n",
    "    _, ax = plt.subplots(figsize=(8,4))\n",
    "    for q in q_list:   \n",
    "        q_time_series, long_q_time_series = req_sketch_example(ts, q, k, period, num_samples)\n",
    "        plot_quantiles(q_time_series, long_q_time_series, q, period, ax)\n",
    "    plt.legend()\n",
    "    ax.set_xlabel('Days')\n",
    "    ax.set_ylabel('Quantile')\n",
    "    ax.set_title(f'Quatile vs. sample number')    \n",
    "\n",
    "        \n",
    "def req_sketch_example(window_samples, q, k, n_periods, num_samples):\n",
    "    window_samples = window_sample(stream_ts, \n",
    "                  length=num_samples, \n",
    "                  stride=num_samples)\n",
    "    \n",
    "    sketches = []\n",
    "    q_time_series = []\n",
    "    long_q_time_series = []\n",
    "    \n",
    "    day_count = 1\n",
    "    for window in window_samples: \n",
    "        kll_sketch = req_floats_sketch(k)\n",
    "        temp_sketch = sketch_build(kll_sketch, window)\n",
    "        sketches.append(temp_sketch)\n",
    "        q_time_series.append(kll_sketch.get_quantile(q))\n",
    "        if day_count % n_periods == 0: \n",
    "            long_sketch, sketches = merge_sketches(sketches)\n",
    "            long_q_time_series.append(long_sketch.get_quantile(q))\n",
    "        day_count += 1\n",
    "    return q_time_series, long_q_time_series\n",
    "\n",
    "# Parameters to compute k\n",
    "eps = 0.05\n",
    "delta = 0.01    \n",
    "num_samples = 7*4*24\n",
    " \n",
    "k = compute_K(eps, delta, num_samples)\n",
    "print(f\"Value of k: {k}\")\n",
    "\n",
    "day_count = 1\n",
    "n_periods = 4\n",
    "q_list = [0.001, 0.999]\n",
    "plot_mulitple_quantiles_req(stream_ts, q_list, k, n_periods, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d6a2cf-cdb7-40b3-ba0c-d6294dd5c894",
   "metadata": {},
   "source": [
    "> The results of the two quantile estimation algorithms are quite similar, which is not terribly surprising since both algorithms use probabilistic sketches optimized to limit relative error. Answer the following questions:\n",
    "> 1. The 4 sketches for both algorithms were merged in a single step. However, one may want to have quantile estimates for several periods. Explain how this can be done and give an example.   \n",
    "> 2. Given the small number of samples in 7 days and given the available [empirical comparison](https://datasketches.apache.org/docs/tdigest/tdigest.html) between the two algorithms is any significant difference in relative error or bias expected and why?\n",
    "> 3. Compare the two values of $k$ computed for the two examples using the REQ algorithm. Explain why they are different.    \n",
    "> 4. How will k change if you reduce the value of $\\epsilon$.    \n",
    "> 5. For a much larger sample sizes, explain how the memory use and relative accuracy of the t-digest and REQ algorithm compare?          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6026b8-30af-43de-935e-65bde85be080",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.        \n",
    "> 2.       \n",
    "> 3.      \n",
    "> 4.        \n",
    "> 5.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f868cca",
   "metadata": {},
   "source": [
    "#### Copyright, 2021, 2022, 2023, 2024, 2025, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
