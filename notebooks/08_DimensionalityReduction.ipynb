{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9df8b5",
   "metadata": {},
   "source": [
    "# Assignment 08\n",
    "\n",
    "## Dimensionality Reduction \n",
    "\n",
    "## CSCI E-108\n",
    "\n",
    "### Steve Elston\n",
    "\n",
    "> Instructions: For this assignment you will complete the exercises shown. All exercises involve creating and executing some Python code. Additionally, most exercises have questions for you to answer. You can answer questions by creating a Markdown cell and writing your answer. If you are not familiar with Markdown, you can find a brief tutorial here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b59b65",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "Dimensionality reduction algorithms are widely used in data mining. Human perception of relationships in data is limited beyond a few dimensions. Further, many data mining algorithms produce poor results where when there is significant dependency between the features or variables. In both cases, we can apply dimensionality reduction methods.   \n",
    "\n",
    "In the exercises in this notebook you will gain some experience working with some commonly used dimensionality reduction methods. Specifically, there are two distinct classes of algorithms you will explore:    \n",
    "1. **Dimensionality reduction transformation methods** create operators to map a sample (feature) space to an orthogonal space. Typically the original data can be well-represented in lower dimensions in the orthogonal space. Examples of these methods include principle component analysis (PCA) and singular value decomposition (SVD).    \n",
    "2. **Manifold learning methods** where data in a high dimensional space is mapped onto a lower-dimensional manifold. When the projection is to a low dimension, manifold learning is used to aid visualization of high-dimensional data. Alternatively, manifold learning can be used as a nonlinear map for dimensionality reduction for further analysis.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5ad06",
   "metadata": {},
   "source": [
    "To begin, execute the code in the cell below to import the packages you will need. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3105ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.manifold import SpectralEmbedding, MDS, TSNE\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn import random_projection\n",
    "import umap\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c15c139",
   "metadata": {},
   "source": [
    "## A Synthetic Example\n",
    "\n",
    "To make the ideas of dimensionality reduction clear, we start with an extremely simple example. In this example, dimensionality reduction is applied to bivariate Normally distributed data. The code in the cell below does the following:  \n",
    "1. Generate 500 samples from a bivariate, zero-centered Normal distribution with covariance having a high degree of dependency between the variables:  \n",
    "$$\n",
    "cov = \n",
    " \\begin{bmatrix}\n",
    "   1.0 & 0.9\\\\\n",
    "   0.9 & 1.0\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "2. Print the empirical covariance matrix of the sample.  \n",
    "3. Plot the simulated data values.\n",
    "\n",
    "Execute this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e3149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_normal(X):\n",
    "    X = pd.DataFrame(X, columns=['axis1','axis2'])\n",
    "    _=sns.jointplot(x='axis1', y='axis2', data=X, xlim=(-4.5,4.5), ylim=(-4.5,4.5))\n",
    "\n",
    "cov = [[1, 0.9], [0.9, 1]]\n",
    "np.random.seed(367)\n",
    "Normal_random = np.random.multivariate_normal([0.0,0.0], cov, size=500)\n",
    "print('The emperical covariance')\n",
    "print(np.cov(Normal_random[:,0], Normal_random[:,1]))\n",
    "plot_normal(Normal_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1e8e9",
   "metadata": {},
   "source": [
    "Notice the following aspects of these results:  \n",
    "1. The empirical covariance matrix is very close in values to the covariance matrix used for the simulation.   \n",
    "2. The scatter plot shows considerable dependency between the two variables. \n",
    "3. The marginal distributions of the two variables appear to be close to Normally distributed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3067d",
   "metadata": {},
   "source": [
    "Next, the code in the cell below does the following.  \n",
    "1. A PCA object is instantiated and the data fit.   \n",
    "2. The PCA model is used to transform or project the original data matrix into the new coordinate system.    \n",
    "3. The empirical covariance is computed and printed. \n",
    "4. **Variance ratio** of the two dimensions of the new space is computed and printed. Here, variance ratio is the variance on each dimension of the space divided by the total variance of the data. \n",
    "5. A plot of the projected data values in the new coordinate space is plotted. \n",
    "\n",
    "Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60ac2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normal_pca = PCA().fit(Normal_random)\n",
    "simple_pca = Normal_pca.transform(Normal_random)\n",
    "print('Covariance of the transformed data')\n",
    "print(np.cov(simple_pca[:,0], simple_pca[:,1]))\n",
    "print(f\"\\nThe variance explained ratio = {Normal_pca.explained_variance_ratio_}\")\n",
    "plot_normal(simple_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81a149",
   "metadata": {},
   "source": [
    "The PCA transformation of these data appears to have worked as expected. Notice the following:  \n",
    "1. The diagonal terms of the covariance matrix are significantly different in value, indicating that the first component (axis) projects the majority of the variance of the data.  \n",
    "2. The off-diagonal terms of the covariance matrix are effectively 0, indicating there is no dependency between the variables in the new coordinate system. \n",
    "3. The observation that most of the variability of the projected data are explained by the first component is confirmed by both the variance ratio values and the scatter plot. \n",
    "4. The marginal distributions of the two variables are very close to Normal, but with significantly difference scale or variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008fe1ed",
   "metadata": {},
   "source": [
    "## First Running Example  \n",
    "\n",
    "We will now start working with some simple real-world data. The famous Iris dataset was collected by a botanist named Edgar Anderson around 1935. Subsequently, the dataset became famous in data analysis circles when Ronald A Fisher used it as an example for his seminal 1936 paper on discriminate analysis, one of the first true multivariate statistical methods proposed. By modern standards this data set is small (only 150 samples) and simple (only 4 features), but the simplicity will help in understand the methods at hand.   \n",
    "\n",
    "The code in the cell below loads the data set and transforms it into a de-meaned Pandas data frame with human readable column and species names. Execute this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e98657",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()\n",
    "\n",
    "## Normalize the data values  \n",
    "temp = (iris_data['data'] - iris_data['data'].mean(axis=0)) / iris_data['data'].std(axis=0)\n",
    "\n",
    "## Prepare the data frame \n",
    "target_species = {0:'Setosa',1:'Versicolour',2:'Virginica'}\n",
    "species = [target_species[x] for x in iris_data['target']]\n",
    "iris = pd.DataFrame(temp, columns=['sepal_length','sepal_width','petal_length','petal_width'])\n",
    "iris['species'] = species\n",
    "iris_data = temp\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb481bd",
   "metadata": {},
   "source": [
    "Since there are only 4 features in this dataset a pairs plot will help with understanding the relationships in these data. Execute the code below to display the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e913be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=sns.pairplot(iris, hue='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cbd801",
   "metadata": {},
   "source": [
    "Examine this plot array. You can see that values samples for the Setosa species are well separated. However there is some overlap between samples from Versicolour and Virginica. Further, and more importantly, it appears that these is considerable redundancy in these plots. This leads one to suspect that there is a high dependency between these cases.  \n",
    "\n",
    "We can further investigate the dependency between the variables by computing the covariance matrix. Execute the code in the cell below to compute the covariance matrix of the iris data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f3f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_convariance = pd.DataFrame(np.cov(np.transpose(iris_data)), index=list(iris.columns)[:-1], columns=list(iris.columns)[:-1])\n",
    "print('Iris covariance matrix')\n",
    "print(iris_convariance)\n",
    "sns.heatmap(iris_convariance, cmap='coolwarm');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5384f43",
   "metadata": {},
   "source": [
    "Several of the off-diagonal terms of the covariance matrix are far from zero. We can conclude that there is significant dependency between the variables.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1943a",
   "metadata": {},
   "source": [
    "## Compute PCA of the iris data   \n",
    "\n",
    "The first algorithm you will apply to the iris data is linear PCA.  \n",
    "\n",
    "> **Exercise 08-1:** Compute the PCA of the iris data and plot the explained variance of the components by the following steps:  \n",
    "> 1. Instantiate a Scikit-learn PCA model object with [sklearn.decomposition.PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).  \n",
    "> 2. Fit the model to `iris_data` numpy array using the `fit` method on the model object.  \n",
    "> 3. Display a scatterplot of with the `explained_variance_ratio_` attribute of the fitted model vs. the component number.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f990fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variance_ratio(pca_model, n_components):   \n",
    "    _=plt.scatter(range(1, n_components + 1), pca_model.explained_variance_ratio_)    \n",
    "    _=plt.hlines(0,1,n_components, color='red')\n",
    "    _=plt.xlabel('Component number')\n",
    "    _=plt.ylabel('Explain variance ratio')\n",
    "    _=plt.title('Explained variance ratio vs. component number')\n",
    "\n",
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "## Display the results\n",
    "n_components = 4\n",
    "plot_variance_ratio(iris_pca, n_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b80e304",
   "metadata": {},
   "source": [
    "> Examine the plot and answer these questions: \n",
    "> 1. Does it appear that much of the variance in the data is explained by the first component and why?  \n",
    "> 2. Is there any substantial difference in the variance explained between the second and third and fourth components?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4020834",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.          \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43125d9d",
   "metadata": {},
   "source": [
    "> Recall that the variance of the components from the PCA goes as the square of the singular values. You can gain another view of the relationship between the principle components by executing the code below to plot the singular values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5cf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_=plt.scatter(range(1, len(iris_pca.singular_values_) + 1), iris_pca.singular_values_)\n",
    "_=plt.hlines(0,1,len(iris_pca.singular_values_), color='red')\n",
    "_=plt.xlabel('Component number')\n",
    "_=plt.ylabel('Singular value')\n",
    "_=plt.title('Singular value vs. component number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b315e80d",
   "metadata": {},
   "source": [
    "> 3. Are these singular values consistent with the variance explained for the components?     \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679b9df",
   "metadata": {},
   "source": [
    "> **Answer 3:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd39cce",
   "metadata": {},
   "source": [
    "Next, you will investigate the principle components used to project the data into the new space. Execute the code in the cell below to print the components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91e7948",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = iris_pca.components_\n",
    "components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4d5aa",
   "metadata": {},
   "source": [
    "The components are in the columns of this array. These components are the projections of the origianl sample space onto the orthogonal space.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3825b",
   "metadata": {},
   "source": [
    "> **Exercise 08-2:** The principle components must be unitary, unit norm, and orthogonal. Do the following to verify these properties.  \n",
    "> 1. In the first cell below compute and print the Euclidean norm of these of the components using [numpy.linalg.norm](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html).    \n",
    "> 2. Using [itertools.combinations](https://docs.python.org/3/library/itertools.html) compute the dot (inner) product of each of pairwise combination of the components using [numpy.dot](https://numpy.org/doc/stable/reference/generated/numpy.dot.html) in the second cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b524253",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The euclidean norm of the components:')\n",
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b7c517",
   "metadata": {},
   "source": [
    "> Examine these results. Are these components orthogonal and unitary and how can you tell?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852b00d2",
   "metadata": {},
   "source": [
    "> **Answers:**       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a0f4e",
   "metadata": {},
   "source": [
    "> **Exercise 08-3:** From the initial exploration of the variance explained and singular values it is the case that a few components can explain most of the variance. To project the 4-dimensional data space to a lower dimensional space do the following:   \n",
    "> 1. Examine the break in the curve for both the explained variance and singular values. This breakpoint determines the number of components you should use for the projection. \n",
    "> 2. Instantiate the projected data array using a PCA model object, with the `n_components` arguments and apply the `fit_transform` method on the `iris_data`.   \n",
    "> 3. Plot the transformation of the data with the `plot_pca` function provided. Make sure you save the returned data frame as `pca_projected`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cca30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca(X, labels, ax=None):\n",
    "    pca_projected = pd.DataFrame(X, columns=['Component_1','Component_2'])\n",
    "    pca_projected['labels'] = labels \n",
    "    if ax == None: \n",
    "        sns.scatterplot(data=pca_projected, x='Component_1', y='Component_2', hue='labels')\n",
    "    else:       \n",
    "        sns.scatterplot(data=pca_projected, x='Component_1', y='Component_2', hue='labels', ax=ax)\n",
    "    return pca_projected\n",
    "\n",
    "## Put your code below   \n",
    "\n",
    "\n",
    "## Plot and save the results \n",
    "pca_projected = plot_pca(iris_pca, species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e5d74",
   "metadata": {},
   "source": [
    "> Examine the plot you have created. Answer the following questions:  \n",
    "> 1. How well can these clusters be linearly separated and thereby separating the specie classes and why?  \n",
    "> 2. Notice the different scales of the two projected variables. Is the range of values of the components consistent with the variance of the components?    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f728080",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.        \n",
    "> 2.            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846d8b2",
   "metadata": {},
   "source": [
    "> We can check the independence of the components by computing the covariance. In the cell below create and execute code to . to display the covariance of the projected components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72586899",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea0a8b",
   "metadata": {},
   "source": [
    "> 3. Notice the small values of the off-diagonal components. Do these values indicate the components are orthogonal.     \n",
    "> **End of Exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccbbfdc",
   "metadata": {},
   "source": [
    "> **Answer 3:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedef94",
   "metadata": {},
   "source": [
    "## Second Example Dataset\n",
    "\n",
    "The bowl disease gene dataset has high dimensionality, with over 10,000 features. The question is, can this high dimensional space be projected to a lower dimensional space?   \n",
    "\n",
    "Execute the code in the cell below to load the data set and prepare it for analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba128c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.read_csv('../data/ColonDiseaseGeneData-Cleaned.csv')\n",
    "labels = gene_data.loc[:,'Disease State']\n",
    "gene_data = gene_data.drop('Disease State', axis=1)\n",
    "\n",
    "## Normalize the columns \n",
    "gene_data = (gene_data - gene_data.mean(axis=0)) / gene_data.std(axis=0)\n",
    "\n",
    "## Display the results \n",
    "print('Shape of the data array = ' + str(gene_data.shape))\n",
    "print(gene_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540bb72",
   "metadata": {},
   "source": [
    "For the 97 subjects there are gene expression values for over 10,000 genes. The limited number of sanples and extreme high dimensionality markes this a challenging problem.     \n",
    "\n",
    "To get a feel for these data, execute the code in the cell below to create and display a UMAP embedding of the gene data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912c7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4365)\n",
    "gene_embedding = umap.UMAP().fit_transform(gene_data)\n",
    "\n",
    "gene_embedding_df = pd.DataFrame(gene_embedding, columns = ['component1', 'component2'])\n",
    "gene_embedding_df['disease'] = [0 if label=='Ulcerative Colitis (UC)' else 1 for label in labels]  \n",
    "\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "sns.scatterplot(data=gene_embedding_df, x='component1', y='component2', hue='disease', ax=ax);\n",
    "ax.set_xlabel('component 1');\n",
    "ax.set_ylabel('component 2');\n",
    "ax.set_title('UMAP projection of standardized gene dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2de46d",
   "metadata": {},
   "source": [
    "On the plot, the disaese state is coded as 1 for Crohn's Disease and 0 for Ulcerative Colitis. By examining the plot is apparent that there is one grouping of patients with Crohn's Disease that is isolated from the others. There is another grouping where patients with both conditions appear mixed. These observations confirm that separating this second group by disease is likely to be quite challenging.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0175ff1e",
   "metadata": {},
   "source": [
    "## PCA with Gene Data    \n",
    "\n",
    "> **Exercise 08-4:** You will now explore the ability of PCA to reduce the dimensionality of the genetics data. To test this idea do the following:   \n",
    "> 1. Instantiate a PCA object and apply the `fit` method with the `gene_data` as the argument.  \n",
    "> 2. Print the cumulative sum of the variance explained by applying the [numpy.cumsum](https://numpy.org/doc/stable/reference/generated/numpy.cumsum.html) function to the `explained_variance_ratio_` attribute of the model object. \n",
    "> 2. Plot the first 60 components of the `explained_variance_ratio_` attribute of the model object vs. the component number. \n",
    "> 3. Execute your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2691223",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Display the explained variance ratio vs. component \n",
    "n_components = 60\n",
    "_=plt.scatter(range(1, n_components + 1), gene_pca.explained_variance_ratio_[:n_components])    \n",
    "_=plt.hlines(0,1,n_components, color='red')\n",
    "_=plt.xlabel('Component number')\n",
    "_=plt.ylabel('Explain variance ratio')\n",
    "_=plt.title('Explained variance ratio vs. component number')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a6be51",
   "metadata": {},
   "source": [
    "> Study your plot. Notice that the explained variance ratio decreases rapidly with the component number. Answer the following questions:  \n",
    "> 1. From the cumulative sums of the variance explained, approximately how much of the total variance can be explained by the first 39 components?  \n",
    "> 2. Does the decay of the variance explained curve indicate that significant dimensionality reduction is possible for these data?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95488869",
   "metadata": {},
   "source": [
    "> **Answers:**    \n",
    "> 1.            \n",
    "> 2.               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce087af",
   "metadata": {},
   "source": [
    "> Now you will display and examine a pairwise scatter plot of the first components of the PCA decomposition of the genetics data. Execute the code below with the number of components representing a cumulative 60% of the variance.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d02457",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Display the components\n",
    "n_components = 8 \n",
    "gene_components = ['Component_1','Component_2','Component_3','Component_4','Component_5','Component_6','Component_7','Component_8']\n",
    "gene_pca_8 = PCA(n_components=n_components).fit(gene_data)\n",
    "gene_pca_projected = pd.DataFrame(gene_pca_8.transform(gene_data), columns=gene_components)\n",
    "gene_pca_projected['Disease'] = labels\n",
    "_=sns.pairplot(gene_pca_projected, hue='Disease')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96d2a5",
   "metadata": {},
   "source": [
    "> Examine the plot. Notice that most of the component values of the two disease types have significant overlap. However, in some cases there are differences in the values on the scatter plots and the marginal density plots. What do these relationship indicate about the ability of these components to separation of the two disease conditions?      \n",
    "> **End of exercise.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d23c6",
   "metadata": {},
   "source": [
    "> **Answer 3:**           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fddc19",
   "metadata": {},
   "source": [
    "### K-means with PCA    \n",
    "\n",
    "Clustering these genetic data is extremely challenging, given the high dimensionality, the small number of genes expressing Crohn's disease and the general complexity. We should therefore have realistic expectations for the results.    \n",
    "\n",
    "The projection of gene data features onto the orthogonal PCA space, can help with creating a better cluster model. In this case, we will create a k-means cluster model using the first 39 components.     \n",
    "\n",
    "As a first step, we need to determine how many clusters there should be. To do so, execute the code in the cell below and examine the resulting plots.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6bfa62-17f4-4ea9-bca4-1b91d0dda176",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_search_kmeans(df, nclusts=(2,21)):\n",
    "    ## If there are cluster assignments in the data frame remove them. \n",
    "    if 'cluster_assignments' in df.columns: df.drop(columns='cluster_assignments', inplace=True)\n",
    "    WCSS=[]\n",
    "    BCSS=[]\n",
    "    silhouette=[]\n",
    "    CH_index = []\n",
    "    ## Compute total sum of squares\n",
    "    x_bar = np.mean(df, axis=0)\n",
    "    TSS = np.sum(np.sum(np.square(df - x_bar)))\n",
    "    for n in range(nclusts[0],nclusts[1]+1):   \n",
    "        temp_model = KMeans(n_clusters=n, n_init=10).fit(df) \n",
    "        WCSS.append(temp_model.inertia_)\n",
    "        BCSS.append(TSS - temp_model.inertia_)\n",
    "        assignments = temp_model.predict(df)\n",
    "        silhouette.append(silhouette_score(df, assignments))\n",
    "        CH_index.append(calinski_harabasz_score(df, assignments))\n",
    "    _, ax = plt.subplots(2,2, figsize=(8,8))    \n",
    "    ax = ax.flatten()\n",
    "    ax[0].plot(range(nclusts[0],nclusts[1]+1),WCSS)   \n",
    "    ax[0].set_xlabel('Number of clusters')\n",
    "    ax[0].set_ylabel('WCSS')\n",
    "    ax[1].plot(range(nclusts[0],nclusts[1]+1),BCSS)   \n",
    "    ax[1].set_xlabel('Number of clusters')\n",
    "    ax[1].set_ylabel('BCSS')\n",
    "    ax[2].plot(range(nclusts[0],nclusts[1]+1),silhouette)   \n",
    "    ax[2].set_xlabel('Number of clusters')\n",
    "    ax[2].set_ylabel('Silhouette Score')\n",
    "    ax[3].plot(range(nclusts[0],nclusts[1]+1),CH_index)   \n",
    "    ax[3].set_xlabel('Number of clusters')\n",
    "    ax[3].set_ylabel('Calinski Harabasz Score')\n",
    "     \n",
    "    \n",
    "n_components = 39   \n",
    "gene_pca_transform = gene_pca.transform(gene_data)\n",
    "gene_pca_transform = pd.DataFrame(gene_pca_transform[:,:n_components], columns=[str(i) for i in range(n_components)])\n",
    "    \n",
    "np.random.seed(9686)\n",
    "cluster_search_kmeans(gene_pca_transform, nclusts=(2,20)) #, label_column='Disease')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6598fd56",
   "metadata": {},
   "source": [
    "The question now is, how to interpret these plots. There is no clear 'peak' in the Calinski-Harabasz index, except at 2 clusters. 2 clusters just represnts the grouping of the values already observed, and is not particularly interesting. The peak at 10 clusters in the silhouette coefficient plot is more promising. Additionally, there are small, but noticable, breaks in the curves of the Calinski-Harabasz index, WCSS and BCSS at 10 clusters. We will continue to explore a model with 10 clusters. To creste and evaluate the 10-cluster model, execute the code in the cell below.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_assignments(df):    \n",
    "    fig,ax = plt.subplots(figsize=(7,6))\n",
    "    sns.scatterplot(data=df, x='component1', y='component2', \n",
    "                    hue='cluster_assignments', style='disease', \n",
    "                    markers=['o','^'], palette=\"Paired\", ax=ax) \n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    ax.set_xlabel('component 1');\n",
    "    ax.set_ylabel('component 2');\n",
    "    ax.set_title('UMAP projection of cluster assignments for HR dataset');\n",
    "\n",
    "n_clustsers=10 #4\n",
    "n_components=60   \n",
    "if 'labels' in gene_pca_transform.columns: gene_pca_transform.drop('labels', axis=1, inplace=True)\n",
    "gene_embedding_df['cluster_assignments'] = KMeans(n_clusters=n_clustsers, n_init=10).fit_predict(gene_pca_transform)\n",
    "\n",
    "plot_cluster_assignments(gene_embedding_df)\n",
    "\n",
    "gene_embedding_df.loc[:,['cluster_assignments','disease']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5e6937",
   "metadata": {},
   "source": [
    "This model shows some interesting relationships. Several clusters have only cases with Corhn's disease, coded 1, and others have only cases of colitis. The other clusters are mixed between the two diseases. Notice that several clusters with Corhn's are in a well separated group away for most cases of colitis. It is likely that considerable domain knowledge is required to go further with the interpretation of this model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeabbe0",
   "metadata": {},
   "source": [
    "## Manifold Learning  \n",
    "\n",
    "Manifold learning seeks to map high-dimensional data onto a low-dimensional linear or nonlinear manifold. There are two reasons to apply manifold learning.      \n",
    "1. To create a low-dimensional projection, $\\le 4$, to aid in visualization of complex data relationships.\n",
    "2. To reduce dimensionality to improve performance of machine learning models, such as unsupervised clustering models.  \n",
    "\n",
    "To start our exploration of manifold learning we will map to a two dimensional manifold which can be displayed as a plot showing relationships in complex, high dimensional data. \n",
    "\n",
    "The Scikit-Learn package contains a large number of linear and nonlinear [manifold learning algorithms](https://scikit-learn.org/stable/modules/manifold.html). Here we will only investiage two possibilities, spectral manifold learning and the [UMAP algorithm](https://umap-learn.readthedocs.io/en/latest/index.html).   \n",
    "\n",
    "> **Manifold leaning is not clustering!** It is a common misconception that manifold learning models are clustering models. While it is the case that manifold projections of complex data can show grouping of observations, these groupings must not be confused with clusters. In contrast, clustering algorithms compute specific cluster assignments to the observations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46e261a",
   "metadata": {},
   "source": [
    "### Spectral Manifold Learning \n",
    "\n",
    "> **Exercise 08-5:** You will now apply the spectral manifold learning to the iris dataset by these steps:   \n",
    "> 1. Instantiate a [sklearn.manifold.SpectralEmbedding](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding) object with argument `affinity='rbf'`, radial basis function.\n",
    "> 2. Use the `fit_transform` method with the iris data as the argument. \n",
    "> 3. Display the result using the `plot_pca` function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d327083",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below \n",
    "\n",
    "\n",
    "\n",
    "pca_projected=plot_pca(iris_spectral, species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01f7154",
   "metadata": {},
   "source": [
    "> Examine this plot. Which aspects (groupings) of the data are well separated?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3121a1d1",
   "metadata": {},
   "source": [
    "> **Answer:**         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844d81ef",
   "metadata": {},
   "source": [
    "### UMAP      \n",
    "\n",
    "You have been using the UMAP algorithm for visualization of clusters in a previous assignment. You will now work with some properties of this Algorithm. As a starting point, execute the code in the cell below to display the embedding computed with the UMAP algorithm with default arguments. This plot will give you a baseline to compare affects of hyperparameters.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca2aeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8273)\n",
    "iris_UMAP = umap.UMAP().fit_transform(iris_data)\n",
    "umap_projected=plot_pca(iris_UMAP, species)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4155c23",
   "metadata": {},
   "source": [
    "There are a number of key hyperparameters that affect the results produced by the UMAP algorithm. Here, we will focus on two, `n_neighbors` and `min_dist`. You can see additional examples of the effects of changing these hyperparamters in the [UMAP documentation](https://umap-learn.readthedocs.io/en/latest/parameters.html). \n",
    "\n",
    "The default values of the two parameters of interest here are, n_neighbors=15 and min_dist=0.1. The code in the cell below tests all possible pairs of hyperparameters that are 0.2 and 5.0 times the default values. The embedding for each hyperparameter pairs is then displayed. Execute this code and examine the result.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25a467",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_dist_list = [0.5, 0.5, 0.02, 0.02]\n",
    "n_neighbor_list = [3, 20, 3, 20]\n",
    "_, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "ax = ax.flatten()\n",
    "np.random.seed(6655)\n",
    "for i, (min_dist, n_neighbors) in enumerate(zip(min_dist_list,n_neighbor_list)):\n",
    "    iris_UMAP = umap.UMAP(min_dist=min_dist, n_neighbors=n_neighbors).fit_transform(iris_data)\n",
    "    umap_projected=plot_pca(iris_UMAP, species, ax=ax[i])\n",
    "    ax[i].set_title('min_dist = ' + str(min_dist) + '  n_neihbors = ' + str(n_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c967695a",
   "metadata": {},
   "source": [
    "> **Exercise 08-6:** Examine these plots. Notice how the clusters change in the embedding. Now answer the following questions:     \n",
    "> 1. How does changing the value of `n_neighbors` change the characteristics of the clusters shown in the embeddings plotted. Why is this behavior expected?          \n",
    "> 2. How and for which plots does the display of the clusters on the embedding with different values of `min_dist`?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f950ba",
   "metadata": {},
   "source": [
    "> **Answers:**       \n",
    "> 1.              \n",
    "> 2.           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db31fff9",
   "metadata": {},
   "source": [
    "### UMAP projection of gene data\n",
    "\n",
    "The UMAP algorithm allows us to **map from a non-Euclidean space to a Euclidean space**. Here we test **mapping from cosine distance to Euclidean distance**. You can now examine the result of changing the hyperparameter values for the embedding of the gene data. As a first step, we execute the code in the cell below to display a baseline projection using cosine distance and the default hyperparameters for `n_neighbors` and `min_dist`.      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd22b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(8833)\n",
    "gene_UMAP = umap.UMAP(metric='cosine').fit_transform(gene_data)\n",
    "umap_projected=plot_pca(gene_UMAP, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb88ee0",
   "metadata": {},
   "source": [
    "Next, execute the code in the cell below to display the embeddings for the pairs of values for `n_neighbors` and `min_dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01acf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbor_list = [10,50,10,50]\n",
    "_, ax = plt.subplots(2,2, figsize=(10,10))\n",
    "ax = ax.flatten()\n",
    "np.random.seed(1010)\n",
    "for i, (min_dist, n_neighbors) in enumerate(zip(min_dist_list,n_neighbor_list)):\n",
    "    gene_UMAP = umap.UMAP(min_dist=min_dist, n_neighbors=n_neighbors, metric='cosine').fit_transform(gene_data)\n",
    "    umap_projected=plot_pca(gene_UMAP, labels, ax=ax[i])\n",
    "    ax[i].set_title('min_dist = ' + str(min_dist) + '  n_neihbors = ' + str(n_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b7d6ab",
   "metadata": {},
   "source": [
    "Examine these plots and notice the following:      \n",
    "1. The small value of `n_neighors` leads to tighter groups. Whereas, the large values of n_neighbors leads to large, poorly separated groups. This represents the fact that the the `n_neighbors` hyperparameter affects how local or wide the search for nearest neighbors is, which affects the properties of the graph used for the UMAP algorithm.      \n",
    "2. The smaller value of `min_dist` yields tighter groups. This is expected since the distance between samples on the manifold is smaller.     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e964887-c8f1-4dfa-b03e-30527e8aba94",
   "metadata": {},
   "source": [
    "### UMAP dimensionality reduction for k-means clustering\n",
    "\n",
    "We will now create a reduced dimensionality manifold projection with UMAP and apply k-means clustering in this Euclidean space. A 20 dimensional space is used for the clustering, which is a reasonable dimensionality for the k-means algorithm.  The code in the cell below does the following:   \n",
    "1. Compute a 20 component embedding using cosine distance with `n_neighbors=10` and `min_dist=0.05`.\n",
    "2. Create a data frame from the projection.\n",
    "3. Call the cluster search function is used over a range of $k=2$ to $k=20$.  \n",
    "Execute the code in the cell below.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16cc2f5-a6f2-4634-8653-2a4a4450e726",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7662)\n",
    "n_components = 20  \n",
    "n_neighbors= 10\n",
    "min_dist = 0.05\n",
    "umap_transform = umap.UMAP(min_dist=min_dist, n_neighbors=n_neighbors, n_components=n_components, metric='cosine')\n",
    "gene_umap_transform = umap_transform.fit_transform(gene_data)\n",
    "gene_umap_transform = pd.DataFrame(gene_umap_transform, columns=[str(i) for i in range(n_components)])\n",
    "    \n",
    "np.random.seed(8686)\n",
    "cluster_search_kmeans(gene_umap_transform, nclusts=(2,20)) #, label_column='Disease')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbede77c-0f8a-42dd-a513-eaeeb6f3ed0f",
   "metadata": {},
   "source": [
    "As is usually the case, finding a value of k in these charts is ambiguous. In this case the Calinski-Harabasz index has a maximum peak at $k=10$. There is a small corresponding peak in the silhouette score and the WCSS curve is flattened out at this point. Given these considerations we will use $k=10$.   \n",
    "\n",
    "The code in the cell below computes a $k=10$ k-means model, using the 20 dimension UMAP manifold embedding, and displays the results. Execute this code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955479c7-590a-4fcf-8944-75cce2b828b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(465)\n",
    "umap_embedding = umap.UMAP(min_dist=min_dist, n_neighbors=n_neighbors, n_components=2, metric='cosine')\n",
    "gene_embedding_df = umap_embedding.fit_transform(gene_data)\n",
    "\n",
    "gene_embedding_df = pd.DataFrame(gene_embedding_df, columns = ['component1', 'component2'])\n",
    "gene_embedding_df['disease'] = [0 if label=='Ulcerative Colitis (UC)' else 1 for label in labels]  \n",
    "\n",
    "\n",
    "n_clustsers=10\n",
    "gene_embedding_df['cluster_assignments'] = KMeans(n_clusters=n_clustsers, n_init=10).fit_predict(gene_umap_transform)\n",
    "\n",
    "plot_cluster_assignments(gene_embedding_df)\n",
    "\n",
    "gene_embedding_df.loc[:,['cluster_assignments','disease']].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37815e3f-e2d8-4428-914e-43ea99fd2031",
   "metadata": {},
   "source": [
    "> **Exercise 8-07:** Examine these results of this model and answer these questions:\n",
    "> 1. Compare the graphs of Calinski-Harabasz and Silhouette score for the k-means clustering with UMAP using cosine metric to the k-means clustering with PCA dimensionality reduction. In one or a few sentences discuss one or two key differences you can observe between these cases.\n",
    "> 2. Compared to the k-means clusters with PCA dimensionality reduction to the k-means clusters with cosine UMAP dimensionality reduction. In one or a few sentences are there any noticeable differences in the cluster assignments?    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d343ba0-8667-47e8-94aa-014a45aeec49",
   "metadata": {},
   "source": [
    "> **Answers:**\n",
    "> 1.      \n",
    "> 2.         "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dd5547",
   "metadata": {},
   "source": [
    "#### Copyright 2021, 2022, 2023, 2024, 2025 Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2850e5a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
